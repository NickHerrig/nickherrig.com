<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Supervision - First Impressions | Nick Herrig</title>
<meta name="keywords" content="computer vision, python, supervision, YOLOv8">
<meta name="description" content="My first impressions while learning the supervision python package from Roboflow.">
<meta name="author" content="Me">
<link rel="canonical" href="http://localhost:1313/posts/supervision/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/supervision/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Nick Herrig (Alt + H)">Nick Herrig</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://resume.nickherrig.com" title="Resume">
                    <span>Resume</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Supervision - First Impressions
    </h1>
    <div class="post-description">
      My first impressions while learning the supervision python package from Roboflow.
    </div>
    <div class="post-meta"><span title='2024-01-14 00:00:00 +0000 UTC'>January 14, 2024</span>&nbsp;Â·&nbsp;Me&nbsp;|&nbsp;<a href="https://github.com/NickHerrig/nickherrig.com/tree/master/content/posts/supervision.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-is-supervision" aria-label="What is Supervision?">What is Supervision?</a></li>
                <li>
                    <a href="#where-to-start" aria-label="Where to start?">Where to start?</a><ul>
                        
                <li>
                    <a href="#installation" aria-label="Installation">Installation</a></li>
                <li>
                    <a href="#video-asset" aria-label="Video Asset">Video Asset</a></li></ul>
                </li>
                <li>
                    <a href="#supervision-features" aria-label="Supervision Features">Supervision Features</a><ul>
                        
                <li>
                    <a href="#detecting-vehicles" aria-label="Detecting Vehicles">Detecting Vehicles</a></li>
                <li>
                    <a href="#annotations" aria-label="Annotations">Annotations</a><ul>
                        
                <li>
                    <a href="#stacking-annotations" aria-label="Stacking Annotations">Stacking Annotations</a></li>
                <li>
                    <a href="#segmentation-vs-detection-annotations" aria-label="Segmentation vs Detection Annotations">Segmentation vs Detection Annotations</a></li>
                <li>
                    <a href="#tracking-and-annotations" aria-label="Tracking and Annotations">Tracking and Annotations</a><ul>
                        
                <li>
                    <a href="#gotcha---stacking-trace-with-segmentation-annotations" aria-label="GOTCHA - Stacking Trace with Segmentation Annotations">GOTCHA - Stacking Trace with Segmentation Annotations</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#tools" aria-label="Tools">Tools</a><ul>
                        
                <li>
                    <a href="#counting-with-line-zones" aria-label="Counting with Line Zones">Counting with Line Zones</a></li>
                <li>
                    <a href="#filtering-with-polygon-zones" aria-label="Filtering with Polygon Zones">Filtering with Polygon Zones</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#first-impressions" aria-label="First Impressions">First Impressions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This year I&rsquo;m starting a new role with <a href="https://roboflow.com/">Roboflow</a>. I&rsquo;ll be helping companies build and deploy computer vision applications. The team, the product, and the mission are all incredible and I&rsquo;m excited to be a part of building the future of computer vision. With this in mind, I thought it&rsquo;d be fun to document my journey as I learn some of the tools and techniques that we use to build computer vision applications &ndash; starting with the <a href="https://github.com/roboflow/supervision">Supervision</a> python package. Here is a sneak peak at some of the things we&rsquo;re going to build and cover.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/WpGcM2zzioc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h1 id="what-is-supervision">What is Supervision?<a hidden class="anchor" aria-hidden="true" href="#what-is-supervision">#</a></h1>
<p>Visiting the <a href="https://supervision.roboflow.com/">Supervision documentation</a> a purple widget reading &ldquo;Ask AI&rdquo; caught my eye. I decided to ask it <code>What is the Supervision python package?</code>.</p>
<blockquote>
<p>&ldquo;Supervision is an open-source Python package developed and maintained by Roboflow. It is designed to facilitate the creation of computer vision applications by providing a range of utilities. The package is distributed under an MIT license, allowing you to use it freely in your projects, whether they are public or private&hellip; &quot;</p>
<ul>
<li><!-- raw HTML omitted -->Ask AI Widget<!-- raw HTML omitted --></li>
</ul>
</blockquote>
<p>The bot went on to describe what you can build with Supervision, and the various utilities available for tracking, annotation, and filtering detections. I was impressed that the project is empracing recent developments of <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models (LLMs)</a> and <a href="https://www.promptingguide.ai/techniques/rag">Retrieval Augments Generation (RAG)</a> inside of the docs to help users quickly find the information they need to guide their project. The bot&rsquo;s response serves as inspiration for some of the experiments in this post.</p>
<h1 id="where-to-start">Where to start?<a hidden class="anchor" aria-hidden="true" href="#where-to-start">#</a></h1>
<p>Before we start playing with the package, let&rsquo;s scaffold our project and install the package. Let&rsquo;s also make sure we have a video to show off some of the features and functionality.</p>
<h2 id="installation">Installation<a hidden class="anchor" aria-hidden="true" href="#installation">#</a></h2>
<p>The first step is to install the Supervision package. This can be done with the following pip command in a terminal of your choice. Personally, I like to first create a virtual environment so that my projects&rsquo; deppendencies are isolated. Let&rsquo;s install the Supervision pre-release version, as it contains some new features that we&rsquo;ll be using.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3 -m venv venv <span style="color:#f92672">&amp;&amp;</span> source venv/bin/activate <span style="color:#f92672">&amp;&amp;</span> pip install <span style="color:#e6db74">&#34;supervision==0.18.0rc1&#34;</span>
</span></span></code></pre></div><h2 id="video-asset">Video Asset<a hidden class="anchor" aria-hidden="true" href="#video-asset">#</a></h2>
<p>Now that we have the package installed, we need a video to test and demo. Luckily for us, the Supervision package has a great selection of high quality <a href="https://supervision.roboflow.com/assets/">video assets</a> that can be used for project ideas and demos. Let&rsquo;s utilize the video of vehicles driving on a highway. You can install the video either with the python package or any other way you&rsquo;d download a video from a link on the internet. I&rsquo;ll be using wget to install the video in my current directory.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://media.roboflow.com/supervision/video-examples/vehicles.mp4 --output vehicles.mp4
</span></span></code></pre></div><p>And voila! We&rsquo;ve got the perfect video for our project. Let&rsquo;s take a look.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/eS7CUVqKLtw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h1 id="supervision-features">Supervision Features<a hidden class="anchor" aria-hidden="true" href="#supervision-features">#</a></h1>
<p>Now that we&rsquo;ve got Supervision installed and a video asset for our project, let&rsquo;s checkout some of the features that the package has to offer. From the documentation there&rsquo;s a lot to play with, including annotators, trackers, and other tools/utilities. Let&rsquo;s start with detecting vehicles in the video.</p>
<h2 id="detecting-vehicles">Detecting Vehicles<a hidden class="anchor" aria-hidden="true" href="#detecting-vehicles">#</a></h2>
<p>In order to detect vehicles we&rsquo;ll need a model, preferably a model that has been trained on vehicle data. Here is where Supervision features start to shine. The package inlcudes a variety of connectors for popular models from <a href="https://github.com/ultralytics/ultralytics">Ultralytics</a>, <a href="https://github.com/facebookresearch/segment-anything">Meta</a>, and <a href="https://github.com/roboflow/inference">Roboflow</a>. So with a single line of code, we can swap out the model we&rsquo;re using in our application ðŸ¤¯. A full list of <a href="https://supervision.roboflow.com/detection/core/">connectors</a> can be found in the documentation. Since we&rsquo;re detecting vehicles, let&rsquo;s use the pretrained <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a> model from <a href="https://www.ultralytics.com/blog/ultralytics-yolov8-turns-one-a-year-of-breakthroughs-and-innovations">Ultralytics</a>. There are a few ways we can do this, but in this project we&rsquo;ll use another Roboflow package called <a href="https://inference.roboflow.com/">Inference</a>. We&rsquo;ll do a deep dive on Inference in the future, but for now just know that it&rsquo;s an open source package that supports running object detection, classification, instance segmentation, and foundational models that also provides a ton of advanced uses and deployment options.</p>
<p>To install Inference run the following command in your terminal.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install inference
</span></span></code></pre></div><p>Now that we have Inference installed, let&rsquo;s write a little bit of code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load the yolov8X object detection model from roboflow inference</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get frames iterable from video and loop over them</span>
</span></span><span style="display:flex;"><span>    frame_generator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>get_video_frames_generator(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> frame <span style="color:#f92672">in</span> frame_generator:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># run inference on the frame</span>
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer(frame)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>        detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span></code></pre></div><p>In the above code we&rsquo;re pulling in the object detection <code>yolov8x-640</code> model from Roboflow Inference, but we could have easily swapped it out for any of our fine tuned models from <a href="https://roboflow.com">Roboflow</a> or <a href="https://universe.roboflow.com/">Roboflow Universe</a> ðŸ˜®.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># rock, paper, scissors model from roboflow universe</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;rock-paper-scissors-sxsw/11&#39;</span>, api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roboflow_private_api_key&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># custom trained model on tricks my golden retriever Ollie can do</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;goldeneye/8&#39;</span>, api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roboflow_private_api_key&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Yolov8 Segmentation model from Roboflow</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-seg-640&#39;</span>)
</span></span></code></pre></div><p>We&rsquo;re also using a fancy utility from the Supervision package called <code>get_video_frame_generator</code> which returns a Python generator that yields frames from our video. Lastly, we&rsquo;re running Inference on each frame with our model and converting the results to a Supervision Detections object.</p>
<h2 id="annotations">Annotations<a hidden class="anchor" aria-hidden="true" href="#annotations">#</a></h2>
<p>At the time of writing this document Supervision supports 15 different annotation types. These include BoundingBox, BoxCorner, Color, Circle, Dot, Triangle, Ellipse, Halo, Mask, Polygon, Label, Blur, Pixelate, Trace, and HeatMap.</p>
<h3 id="stacking-annotations">Stacking Annotations<a hidden class="anchor" aria-hidden="true" href="#stacking-annotations">#</a></h3>
<p>Each annoator has the ability to be stacked; however one interesting finding is that the order may influence the quality of the output. For example, if you were to annotate a bounding box, then stack a pixelated annoation, the bounding box may be pixelated as seen in the image below.</p>
<p><img loading="lazy" src="/images/supervision/blur-bounding-box.webp" alt="blurred-bounding"  />
</p>
<p>You can solve this problem by instead adding the pixelated annotation first, then the bounding box annotation.</p>
<p><img loading="lazy" src="/images/supervision/fixed-blur-bounding-box.webp" alt="fixed-blurred-bounding"  />
</p>
<p>Let&rsquo;s dive into the code for creating these annotations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load the yolov8X model from roboflow inference</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># get video info from the video path and dynamically generate line thickness</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    video_info <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>VideoInfo<span style="color:#f92672">.</span>from_video_path(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex; background-color:#3c3d38"><span>    thickness <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>calculate_dynamic_line_thickness(video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create a bounding box annotator with dynamic thickness and a pixelate annotator</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    bounding_box <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>BoundingBoxAnnotator(thickness<span style="color:#f92672">=</span>thickness)
</span></span><span style="display:flex; background-color:#3c3d38"><span>    pixalate <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>PixelateAnnotator()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get frames iterable from video and loop over them</span>
</span></span><span style="display:flex;"><span>    frame_generator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>get_video_frames_generator(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a video sink context manager to write the annotated frames to</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#66d9ef">with</span> sv<span style="color:#f92672">.</span>VideoSink(target_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output.mp4&#34;</span>, video_info<span style="color:#f92672">=</span>video_info) <span style="color:#66d9ef">as</span> sink:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> frame <span style="color:#f92672">in</span> frame_generator:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># run inference on the frame</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer(frame)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>            detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># apply pixalate on frame copy, then add bounding box</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> pixalate<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>frame<span style="color:#f92672">.</span>copy(), detections<span style="color:#f92672">=</span>detections)
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> bounding_box<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>annotated_frame, detections<span style="color:#f92672">=</span>detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># save the annotated frame to the video sink</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            sink<span style="color:#f92672">.</span>write_frame(frame<span style="color:#f92672">=</span>annotated_frame)
</span></span></code></pre></div><p>We&rsquo;ve introduced a couple of helpful utilities. The <code>VideoInfo</code> object helps us by providing information including frames per second, height, width, etc. We can use this information to dynamically generate annotator line thickness with the <code>sv.calculate_dynamic_line_thickness()</code> method. Next, we create a couple of annotators, a <code>BoundingBoxAnnotator</code> and a <code>PixelateAnnotator</code>. We&rsquo;re also introducing a new concept called a <code>VideoSink</code> which is just a fancy context manager that allows us to write frames to a video output. Prior to this we apply the annotations to the frame with the <code>annotate</code> methods on the annotators. Pretty cool right?</p>
<h3 id="segmentation-vs-detection-annotations">Segmentation vs Detection Annotations<a hidden class="anchor" aria-hidden="true" href="#segmentation-vs-detection-annotations">#</a></h3>
<p>Some annotations require specific model output that depend on the data returned at inference time. More specifically, the Halo, Mask, and Polygon all use <code>sv.Detections.mask</code> under the hood to generate the annotations. This means that we need to use a segmentation model, instead of our object detection model to use these annoations. Let&rsquo;s swap out our model <code>yolov8x-640</code> for <code>yolov8x-seg-640</code> to ensure we recieve a mask property in our detection ojects. Take a look at an example of the Polygon Annotator.</p>
<p><img loading="lazy" src="/images/supervision/poly.webp" alt="polygon-annotator"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># load a segmentation model from roboflow inference instead</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-seg-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><h3 id="tracking-and-annotations">Tracking and Annotations<a hidden class="anchor" aria-hidden="true" href="#tracking-and-annotations">#</a></h3>
<p>The Trace annotator requires the <code>sv.Detections.tracker_id</code> be present to generate annotations. This means that we&rsquo;ll have to use a tracker. Trackers are a piece of code that identifies objects across frames and assigns them a unique id. For example, we could use a tracker to learn what direction a vehicle is moving. There are a few popular trackers at the time of writing this including <a href="https://github.com/ifzhang/ByteTrack">ByteTrack</a> and <a href="https://github.com/NirAharon/BoT-SORT">Bot-SORT</a>. Supervision makes using trackers a breeze and comes with ByteTrack built-in.</p>
<p><img loading="lazy" src="/images/supervision/track-id-trace.webp" alt="track-id-trace"  />
</p>
<p>Let&rsquo;s dive into some tracking code!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load the yolov8X model from roboflow inference</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-seg-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get video info from the video path and dynamically generate line thickness and text_scale</span>
</span></span><span style="display:flex;"><span>    video_info <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>VideoInfo<span style="color:#f92672">.</span>from_video_path(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>    thickness <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>calculate_dynamic_line_thickness(video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex; background-color:#3c3d38"><span>    text_scale <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>calculate_dynamic_text_scale(video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create a trace and label annotator, with dynamic video info</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    trace <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>TraceAnnotator(thickness<span style="color:#f92672">=</span>thickness)
</span></span><span style="display:flex; background-color:#3c3d38"><span>    label <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>LabelAnnotator(text_thickness<span style="color:#f92672">=</span>thickness, text_scale<span style="color:#f92672">=</span>text_scale)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create a ByteTrack object to track detections</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    byte_tracker <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>ByteTrack(frame_rate<span style="color:#f92672">=</span>video_info<span style="color:#f92672">.</span>fps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get frames iterable from video and loop over them</span>
</span></span><span style="display:flex;"><span>    frame_generator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>get_video_frames_generator(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a video sink context manager to write the annotated frames to</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> sv<span style="color:#f92672">.</span>VideoSink(target_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output.mp4&#34;</span>, video_info<span style="color:#f92672">=</span>video_info) <span style="color:#66d9ef">as</span> sink:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> frame <span style="color:#f92672">in</span> frame_generator:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># run inference on the frame</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer(frame)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>            detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># update detections with tracker ids</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            tracked_detections <span style="color:#f92672">=</span> byte_tracker<span style="color:#f92672">.</span>update_with_detections(detections)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># apply trace annotator to frame</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>frame<span style="color:#f92672">.</span>copy(), detections<span style="color:#f92672">=</span>tracked_detections)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># create label text for annotator</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            labels <span style="color:#f92672">=</span> [ <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>tracker_id<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#66d9ef">for</span> tracker_id <span style="color:#f92672">in</span> tracked_detections<span style="color:#f92672">.</span>tracker_id ]
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># apply label annotator to frame</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> label<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>annotated_frame, detections<span style="color:#f92672">=</span>tracked_detections, labels<span style="color:#f92672">=</span>labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># save the annotated frame to the video sink</span>
</span></span><span style="display:flex;"><span>            sink<span style="color:#f92672">.</span>write_frame(frame<span style="color:#f92672">=</span>annotated_frame)
</span></span></code></pre></div><p>In this code, we add <code>text_scale</code> as a new dynamic value for labeling the <code>tracker_id</code>. Next, we add a Trace and Label Annotator with these dynamic values. We also create a <code>ByteTrack</code> object to track our detections passing in the video&rsquo;s frame rate. From there, all we need to do is update the detections with our byte_tracker results using the method <code>update_with_detections</code>. Lastly, we create a list of labels from the tracker_ids and pass them to the label annotator while also utilzing the <code>TraceAnnotator</code> to trace the path of each detection.</p>
<h4 id="gotcha---stacking-trace-with-segmentation-annotations">GOTCHA - Stacking Trace with Segmentation Annotations<a hidden class="anchor" aria-hidden="true" href="#gotcha---stacking-trace-with-segmentation-annotations">#</a></h4>
<p>When trying to stack a Trace annotation with a Polygon annotation, I ran into a hiccup. When using a detections from <code>ByteTrack.update_with_detections(detections)</code> the resulting <code>sv.Detections</code> does not include segmentation masks ðŸ˜­. This isn&rsquo;t a show stopper, but there is a little bit of nuance if we&rsquo;d like to stack a Segmentation Annotator with a Trace Annotator.</p>
<p><img loading="lazy" src="/images/supervision/trace-seg.webp" alt="trace-seg"  />
</p>
<p>We need to apply the polygon annotation with the <code>sv.Detections.from_inference()</code> instead of the detections from <code>ByteTrack.update_with_detections(detections)</code>. The former contains the mask and the latter doesn&rsquo;t include the mask. This looks to be a <a href="https://github.com/roboflow/supervision/issues/418">bug</a>, as the latter does include a <code>mask</code> property, it just returns <code>None</code>. More to come on this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># first apply annotation that requires sv.Detections.mask</span>
</span></span><span style="display:flex;"><span>annotated_frame <span style="color:#f92672">=</span> polygon<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>frame<span style="color:#f92672">.</span>copy(), detections<span style="color:#f92672">=</span>detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># then update detections with tracker ids</span>
</span></span><span style="display:flex;"><span>tracked_detections <span style="color:#f92672">=</span> byte_tracker<span style="color:#f92672">.</span>update_with_detections(detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># apply trace annotator to frame</span>
</span></span><span style="display:flex;"><span>annotated_frame <span style="color:#f92672">=</span> trace<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>annotated_frame, detections<span style="color:#f92672">=</span>tracked_detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><p>To wrap up Annotators, let&rsquo;s get a little crazy ðŸ™ˆ. Let&rsquo;s cycle through a few more of the available annotations in a single video to check them all out. Below is the resulting video.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/Hlq3xWWuu0A" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>Annotations are a powerful tool for visualizing detections. Now let&rsquo;s cover some powerful tools that Supervision provides.</p>
<h2 id="tools">Tools<a hidden class="anchor" aria-hidden="true" href="#tools">#</a></h2>
<p>Supervision provides a few additional tools that can be used to filter detections, count detections, create zones, and slice frames. Let&rsquo;s dive into a few of them!</p>
<h3 id="counting-with-line-zones">Counting with Line Zones<a hidden class="anchor" aria-hidden="true" href="#counting-with-line-zones">#</a></h3>
<p>A <a href="https://supervision.roboflow.com/detection/tools/line_zone/">Line Zone</a> can be utilized for counting a number of objects that cross a predefined line. One cool feature of the line is that it keeps track of two attributes: <code>in_count</code> and <code>out_count</code>. These attributes can be used to count the number of objects that cross the line in either direction. Note that the <code>LineZone</code> utilizes the <code>sv.Detections.tracker_id</code> to keep track of objects so make sure you&rsquo;re using a tracker as discussed above. Remember earlier when I said that supervision has 15 annotators? Well, I lied. There are a few more that are only documented in in the best kind of documenation, the code ðŸ˜œ. Let&rsquo;s create a <code>LineZone</code> and show it in action with the <code>LineZoneAnnotator</code>.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/VuFK17KLHpI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>And here&rsquo;s the code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load the yolov8X model from roboflow inference</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8x-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get video info from the video path and dynamically generate line thickness and text_scale</span>
</span></span><span style="display:flex;"><span>    video_info <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>VideoInfo<span style="color:#f92672">.</span>from_video_path(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>    text_scale <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>calculate_dynamic_text_scale(video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a ByteTrack object to track detections</span>
</span></span><span style="display:flex;"><span>    byte_tracker <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>ByteTrack(frame_rate<span style="color:#f92672">=</span>video_info<span style="color:#f92672">.</span>fps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get frames iterable from video and loop over them</span>
</span></span><span style="display:flex;"><span>    frame_generator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>get_video_frames_generator(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create two points of a line for the LineZone</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    start_point <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Point(<span style="color:#ae81ff">0</span>, video_info<span style="color:#f92672">.</span>height<span style="color:#f92672">*</span>(<span style="color:#ae81ff">3</span><span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex; background-color:#3c3d38"><span>    end_point <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Point(video_info<span style="color:#f92672">.</span>width, video_info<span style="color:#f92672">.</span>height<span style="color:#f92672">*</span>(<span style="color:#ae81ff">3</span><span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create the LineZone object</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    line_zone <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>LineZone(start_point, end_point)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create the LineZoneAnnotator</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    line_annotator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>LineZoneAnnotator(
</span></span><span style="display:flex; background-color:#3c3d38"><span>        color<span style="color:#f92672">=</span>sv<span style="color:#f92672">.</span>Color<span style="color:#f92672">.</span>green(),
</span></span><span style="display:flex; background-color:#3c3d38"><span>        text_scale<span style="color:#f92672">=</span>text_scale,
</span></span><span style="display:flex; background-color:#3c3d38"><span>        custom_in_text<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;OUT&#34;</span>,
</span></span><span style="display:flex; background-color:#3c3d38"><span>        custom_out_text<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;IN&#34;</span>,
</span></span><span style="display:flex; background-color:#3c3d38"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a video sink context manager to write the annotated frames to</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> sv<span style="color:#f92672">.</span>VideoSink(target_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output.mp4&#34;</span>, video_info<span style="color:#f92672">=</span>video_info) <span style="color:#66d9ef">as</span> sink:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> frame <span style="color:#f92672">in</span> frame_generator:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># run inference on the frame</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer(frame)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>            detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># update detections with tracker ids</span>
</span></span><span style="display:flex;"><span>            tracked_detections <span style="color:#f92672">=</span> byte_tracker<span style="color:#f92672">.</span>update_with_detections(detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># update the linezone object with detections</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            line_zone<span style="color:#f92672">.</span>trigger(detections<span style="color:#f92672">=</span>detections)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># apply the line zone annotator to the frame</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> line_annotator<span style="color:#f92672">.</span>annotate(frame<span style="color:#f92672">=</span>frame<span style="color:#f92672">.</span>copy(), line_counter<span style="color:#f92672">=</span>line_zone)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># save the annotated frame to the video sink</span>
</span></span><span style="display:flex;"><span>            sink<span style="color:#f92672">.</span>write_frame(frame<span style="color:#f92672">=</span>annotated_frame)
</span></span></code></pre></div><p>When first running this code, I ran into the following issue:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Traceback <span style="color:#f92672">(</span>most recent call last<span style="color:#f92672">)</span>:
</span></span><span style="display:flex;"><span>  File <span style="color:#e6db74">&#34;/Users/nick/git/computer-vision/line_zone.py&#34;</span>, line 41, in &lt;module&gt;
</span></span><span style="display:flex;"><span>    line_zone.trigger<span style="color:#f92672">(</span>detections<span style="color:#f92672">=</span>detections<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  File <span style="color:#e6db74">&#34;/Users/nick/git/computer-vision/venv/lib/python3.11/site-packages/supervision/detection/line_counter.py&#34;</span>, line 57, in trigger
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, <span style="color:#f92672">(</span>xyxy, _, confidence, class_id, tracker_id<span style="color:#f92672">)</span> in enumerate<span style="color:#f92672">(</span>detections<span style="color:#f92672">)</span>:
</span></span><span style="display:flex;"><span>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</span></span><span style="display:flex;"><span>ValueError: too many values to unpack <span style="color:#f92672">(</span>expected 5<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>To fix this issue, I cracked open the supervision package and change the following code in <code>line_counter.py</code> to ignore the data field in the detections object.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> i, (xyxy, _, confidence, class_id, tracker_id, _) <span style="color:#f92672">in</span> enumerate(detections):
</span></span></code></pre></div><p>We&rsquo;re using the early release candidate so expect a couple of bugs. This should be fixed in the next release.
Let&rsquo;s now move on to another tool, the Polygon Zone!</p>
<h3 id="filtering-with-polygon-zones">Filtering with Polygon Zones<a hidden class="anchor" aria-hidden="true" href="#filtering-with-polygon-zones">#</a></h3>
<p>The <a href="https://supervision.roboflow.com/detection/tools/polygon_zone/">Polygon Zone</a> is an object we can utilize for a variety of tasks. We can filter detections, count, and provide logic for a variety of other tasks. Let&rsquo;s use it to filter out detections that are only on the right side of the highway. We&rsquo;ll also utilize the <code>PolygonZoneAnnotator</code> to visualize the polygon zone and keep a count of detections inside of it.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/oz0vdoZ_gTA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>Let&rsquo;s take a peek at the code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> supervision <span style="color:#66d9ef">as</span> sv
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> inference.models.utils <span style="color:#f92672">import</span> get_roboflow_model
</span></span><span style="display:flex; background-color:#3c3d38"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load the yolov8X model from roboflow inference</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_roboflow_model(<span style="color:#e6db74">&#39;yolov8n-640&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get video info from the video path and dynamically generate line thickness and text_scale</span>
</span></span><span style="display:flex;"><span>    video_info <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>VideoInfo<span style="color:#f92672">.</span>from_video_path(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>    text_scale <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>calculate_dynamic_text_scale(video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a ByteTrack object to track detections</span>
</span></span><span style="display:flex;"><span>    byte_tracker <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>ByteTrack(frame_rate<span style="color:#f92672">=</span>video_info<span style="color:#f92672">.</span>fps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get frames iterable from video and loop over them</span>
</span></span><span style="display:flex;"><span>    frame_generator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>get_video_frames_generator(<span style="color:#e6db74">&#39;vehicle.mp4&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create a polygon for use in the PolygonZone using https://roboflow.github.io/polygonzone/</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    polygon <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">1758</span>],[<span style="color:#ae81ff">1125</span>, <span style="color:#ae81ff">846</span>],[<span style="color:#ae81ff">1697</span>, <span style="color:#ae81ff">850</span>],[<span style="color:#ae81ff">1885</span>, <span style="color:#ae81ff">2146</span>],[<span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">2146</span>],[<span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">1754</span>]])
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create the PolygonZone object and PolygonZoneAnnotator</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    polygon_zone <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>PolygonZone(polygon, frame_resolution_wh<span style="color:#f92672">=</span>video_info<span style="color:#f92672">.</span>resolution_wh)
</span></span><span style="display:flex; background-color:#3c3d38"><span>    polygon_annotator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>PolygonZoneAnnotator(color<span style="color:#f92672">=</span>sv<span style="color:#f92672">.</span>Color<span style="color:#f92672">.</span>green(), zone<span style="color:#f92672">=</span>polygon_zone, text_scale<span style="color:#f92672">=</span>text_scale)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    <span style="color:#75715e"># create a box annotator to visualize detections inside the polygon zone</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>    box_annotator <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>BoxAnnotator(text_scale<span style="color:#f92672">=</span>text_scale)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create a video sink context manager to write the annotated frames to</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> sv<span style="color:#f92672">.</span>VideoSink(target_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;polygonzone.mp4&#34;</span>, video_info<span style="color:#f92672">=</span>video_info) <span style="color:#66d9ef">as</span> sink:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> frame <span style="color:#f92672">in</span> frame_generator:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># run inference on the frame</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>infer(frame)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># convert the detections to a supervision detections object</span>
</span></span><span style="display:flex;"><span>            detections <span style="color:#f92672">=</span> sv<span style="color:#f92672">.</span>Detections<span style="color:#f92672">.</span>from_inference(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># filter based on the polygon zone</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            detections <span style="color:#f92672">=</span> detections[polygon_zone<span style="color:#f92672">.</span>trigger(detections)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># update detections with tracker ids</span>
</span></span><span style="display:flex;"><span>            tracked_detections <span style="color:#f92672">=</span> byte_tracker<span style="color:#f92672">.</span>update_with_detections(detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># update the polygon zone with detections for count</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            polygon_zone<span style="color:#f92672">.</span>trigger(tracked_detections)
</span></span><span style="display:flex; background-color:#3c3d38"><span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            <span style="color:#75715e"># show the polygon zone and box annotator</span>
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> polygon_annotator<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>frame<span style="color:#f92672">.</span>copy())
</span></span><span style="display:flex; background-color:#3c3d38"><span>            annotated_frame <span style="color:#f92672">=</span> box_annotator<span style="color:#f92672">.</span>annotate(scene<span style="color:#f92672">=</span>annotated_frame, detections<span style="color:#f92672">=</span>tracked_detections)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># save the annotated frame to the video sink</span>
</span></span><span style="display:flex;"><span>            sink<span style="color:#f92672">.</span>write_frame(frame<span style="color:#f92672">=</span>annotated_frame)
</span></span></code></pre></div><p>Holy cow that&rsquo;s pretty sweet! We&rsquo;re using the <code>PolygonZone</code> and <code>PolygonZoneAnnotator</code> to both filter detections and count the vehicles in the zone. You can start to see all the possibilities of using custom models and a variety of polygon zones.</p>
<h1 id="first-impressions">First Impressions<a hidden class="anchor" aria-hidden="true" href="#first-impressions">#</a></h1>
<p>It&rsquo;s clear that the package is moving quickly and features are changing rapidly. The documentation is currently lagging behind, so sometimes you need to dive into the code to see what&rsquo;s new. As I started to play with the tool it&rsquo;s easy to see how custom models and this tooling can help developers make the world programmable. It&rsquo;s incredible what you can do in a few lines of code with the Supervision package. I&rsquo;m excited to try out some of the other improvements to Supervision as they are released. The future of computervision is bright and it&rsquo;s only the beginning. Cheers to the future ðŸ».</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/computer-vision/">Computer Vision</a></li>
      <li><a href="http://localhost:1313/tags/python/">Python</a></li>
      <li><a href="http://localhost:1313/tags/supervision/">Supervision</a></li>
      <li><a href="http://localhost:1313/tags/yolov8/">YOLOv8</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">Nick Herrig</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
