[{"content":"It\u0026rsquo;s been 2 days since the launch of Llama 3 and I\u0026rsquo;ve been itching to try it out. Here is a short blog post experimenting with ollama to locally run this sota(state of the art) llm(large language model). But first, what is llama3?\nWhat is Llama3 Meta\u0026rsquo;s Llama 3 is the most capable openly available LLM as of writing this blog. On April 18th 2024 Meta released two models of this generation, A 8B and 70B parameter model. Below are a few model performance charts on various LLM benchmarks.\nLlama3 Performance One thing that stood out to me is that both the 8B and 70B sizes match or exceed similar class models. The exciting difference though, is that I can run the 70B parameter model locally on my laptop, while this isn\u0026rsquo;t even an option for similar performing models like Google Gemini Pro 1.5, Claude 3 Sonnet, or Open AI\u0026rsquo;s GPT-4. Meta also is actively training a larger 400B parameter model, which will likely out perform these two releases. Very exciting time to be alive!\nüî•üî•üî•\nLet\u0026rsquo;s take a crack at getting this thing running locally. To do this let\u0026rsquo;s use Ollama.\nWhat is Ollama Ollama is the easiest tooling I\u0026rsquo;ve discovered for getting LLMs up and running locally on any hardware. It can be installed on MacOS, Windows, and Linux by following instructions on their github repo, which boasts an impressive 57k stars.\nPulling the Models I\u0026rsquo;m currently working on a mac, and to download the 7B parameter llama3 model I run the following command.\nollama pull llama3 pulling manifest pulling 00e1317cbf74... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.7 GB pulling 4fa551d4f938... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 12 KB pulling 8ab4849b038c... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 254 B pulling c0aac7c7f00d... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 128 B pulling db46ef36ef0b... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 483 B verifying sha256 digest writing manifest removing any unused layers success Just like that we downloaded the model weights. Downloading the 70B parameter model takes a little longer since it\u0026rsquo;s 40GB, a little larger than the ~5GB 7B model. The 70B model can be pulled with ollama pull llama3:70b.\nChatting with the Model Now that we have the model weights installed locally let\u0026rsquo;s chat with it. To run a chat session from the terminal we run ollama run {model}. Let\u0026rsquo;s chat with the 70B parameter model!\nollama run llama3:70b \u0026gt;\u0026gt;\u0026gt; Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? A classic lateral thinking puzzle! At first glance, it seems like a straightforward question: \u0026#34;How many sisters does Sally have?\u0026#34; But wait... If each of Sally\u0026#39;s 3 brothers has 2 sisters, that means each brother has Sally and one other sister. So, in total, there is only 1 other sister besides Sally. Therefore, Sally has exactly 1 sister. \u0026gt;\u0026gt;\u0026gt; Send a message (/? for help) Not bad! Note, that when asking this same question to Mistral 7B the model incorrectly answers 5.\nollama run mistral \u0026gt;\u0026gt;\u0026gt; Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Let\u0026#39;s break this down: 1. We know that Sally is a girl, and she has 3 brothers. 2. Each of her brothers has 2 sisters. So each brother is sharing those 2 sisters with Sally and the other brothers. 3. Since there are three brothers, and each brother has 2 sisters, then in total, they have 2 * 3 = 6 sisters. However, since we already know that Sally is one of them, we need to subtract her from the count as she was not included in the \u0026#34;brothers\u0026#34; count. 4. So, Sally has 6 - 1 = 5 sisters. \u0026gt;\u0026gt;\u0026gt; Send a message (/? for help) I don\u0026rsquo;t know about you, but I\u0026rsquo;m looking forward to see how applications incorporate these powerful and accessible open source models into their business model. Ollama already boasts a impressive list of integrations that all have access to run these meta llama3 models. What will you build with Meta Llama 3? ü§îüí°\n","permalink":"http://localhost:1313/posts/local-llama3/","summary":"It\u0026rsquo;s been 2 days since the launch of Llama 3 and I\u0026rsquo;ve been itching to try it out. Here is a short blog post experimenting with ollama to locally run this sota(state of the art) llm(large language model). But first, what is llama3?\nWhat is Llama3 Meta\u0026rsquo;s Llama 3 is the most capable openly available LLM as of writing this blog. On April 18th 2024 Meta released two models of this generation, A 8B and 70B parameter model.","title":"Local Llama3 with Ollama"},{"content":"Today I was playing around with an LLM called Mistral 7B by running it locally with Ollama. Once installed, Ollama provides a chat interface and an API that you can use and run where ever.\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;tell me a joke?\u0026#34; }\u0026#39; When running this api call, I noticed that responses were streamed back to the client in a way that appears to be token by token. Take a look at running the command.\nStreaming API Endpoint This got me thinking a little bit about how this would work with the Python requests library. I wrote the code I usually would to interact with a post request as seen below, wondering if this would workout.\nimport requests url = \u0026#39;http://localhost:11434/api/generate\u0026#39; data = { \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34; } response = requests.post(url, json=data) print(response.json()) Boom üí•, broken. It looks like there are some JSONDecodeErrors. Checkout the error below.\npython3 main.py Traceback (most recent call last): File \u0026#34;/Users/nickherrig/git/llms/venv/lib/python3.10/site-packages/requests/models.py\u0026#34;, line 960, in json return complexjson.loads(self.content.decode(encoding), **kwargs) File \u0026#34;/Users/nickherrig/.pyenv/versions/3.10.13/lib/python3.10/json/__init__.py\u0026#34;, line 346, in loads return _default_decoder.decode(s) File \u0026#34;/Users/nickherrig/.pyenv/versions/3.10.13/lib/python3.10/json/decoder.py\u0026#34;, line 340, in decode raise JSONDecodeError(\u0026#34;Extra data\u0026#34;, s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 92) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \u0026#34;/Users/nickherrig/git/llms/main.py\u0026#34;, line 11, in \u0026lt;module\u0026gt; print(response.json()) File \u0026#34;/Users/nickherrig/git/llms/venv/lib/python3.10/site-packages/requests/models.py\u0026#34;, line 968, in json raise RequestsJSONDecodeError(e.msg, e.doc, e.pos) requests.exceptions.JSONDecodeError: Extra data: line 2 column 1 (char 92) I did a bit of digging in the requests library documentation, and you can set stream=True in the request.post() portion of the code. From there, response has a method iter_lines() which you can loop over as a context window and returns a line of \u0026lt;class 'bytes'\u0026gt;. Let\u0026rsquo;s take a look at the updated code.\nimport requests import json url = \u0026#39;http://localhost:11434/api/generate\u0026#39; data = { \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Tell me a Joke?\u0026#34; } response = requests.post(url, json=data, stream=True) for line in response.iter_lines(): print(type(line)) if line: decoded_line = line.decode(\u0026#39;utf-8\u0026#39;) print(json.loads(decoded_line)) In this script, I\u0026rsquo;m simply printing the json response, as it\u0026rsquo;s streamed in. Ideally, I\u0026rsquo;d like to see each word/token streamed to std out as they are recieved. This took a little experimenting, but ended up finding two parameters that are part of the print() statement; end and flush. By default end is set to a newline character, which makes sense. Since we\u0026rsquo;re going after streaming responses word by word, we\u0026rsquo;ll set this to an empty string \u0026quot;\u0026quot;. By default flush is set to False meaning that output is written to the consol under certain conditions, such as when the buffer is full, a newline character is encountered, or the program finishes execution.\nLet\u0026rsquo;s take a look at the updated code.\nimport requests import json url = \u0026#39;http://localhost:11434/api/generate\u0026#39; data = { \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;write me a story about python.\u0026#34; } response = requests.post(url, json=data, stream=True) for line in response.iter_lines(): if line: decoded_line = line.decode(\u0026#39;utf-8\u0026#39;) print(json.loads(decoded_line)[\u0026#34;response\u0026#34;], end=\u0026#34;\u0026#34;, flush=True) And let\u0026rsquo;s see it in action!\nStreaming with Python Happy streaming! üî•\n","permalink":"http://localhost:1313/posts/streaming-requests/","summary":"Today I was playing around with an LLM called Mistral 7B by running it locally with Ollama. Once installed, Ollama provides a chat interface and an API that you can use and run where ever.\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;tell me a joke?\u0026#34; }\u0026#39; When running this api call, I noticed that responses were streamed back to the client in a way that appears to be token by token.","title":"Streaming LLM Requests with Python"},{"content":"I recently started a new course called Practical Deep Learning. The course is taught by Jeremy Howard, a machine learning legend and founder of fast.ai. One of the first lessons in the course was training a classifier to detect birds in images. As part of learning the materials, I thought i\u0026rsquo;d be a fun experiment to extend this model to classify the top 10 most popular cocktails. Let\u0026rsquo;s get started üç∏üçπ.\nWhat we\u0026rsquo;re building The goal is to identify the most popular cocktails in images. We\u0026rsquo;ll be focusing on the below 10 cocktails.\nOld Fashioned Margarita Mojito Pina Colada Manhattan Whiskey Sour Gin \u0026amp; Tonic Long Island Iced Tea Cosmopolitan Daiquiri In order to accomplish this goal, we\u0026rsquo;ll be fine tuning a resnet18 model on images collected via the DuckDuckGo search engine. From there, we\u0026rsquo;ll feed the model our images to see how it performs.\nInstalling our Dependencies Before we get started let\u0026rsquo;s pip install our python packages from fast.ai and DuckDuckGo.\npip install -U fastai duckduckgo_search Now that we have our packages installed let\u0026rsquo;s collect our data.\nRetrieving the Data In order to train the model, we need some images of cocktails. Luckily, DuckDuckGo provides a nice way to do this. We\u0026rsquo;ll use a python package called duckduckgo_search to accomplish fetching some images for training. Let\u0026rsquo;s define a function that takes a search term and max_images and returns a list of image urls for download\nfrom duckduckgo_search import DDGS from fastcore.all import L def search_images(term, max_images=200): return L(DDGS().images(term, max_results=max_images)).itemgot(\u0026#39;image\u0026#39;) print(search_images(\u0026#34;margarita\u0026#34;, max_images=1)) as you can see running this script returns an image of a margarita\npython3 download.py [\u0026#39;https://downshiftology.com/wp-content/uploads/2019/05/Margarita-9.jpg\u0026#39;] Let\u0026rsquo;s collect 100 images of each drink type for training. We\u0026rsquo;ll also cleverly create drink directories so that we can use the parent directory name for training later.\nTraining the Model Testing the Model Streaming API Endpoint ","permalink":"http://localhost:1313/posts/drink-classifier/","summary":"I recently started a new course called Practical Deep Learning. The course is taught by Jeremy Howard, a machine learning legend and founder of fast.ai. One of the first lessons in the course was training a classifier to detect birds in images. As part of learning the materials, I thought i\u0026rsquo;d be a fun experiment to extend this model to classify the top 10 most popular cocktails. Let\u0026rsquo;s get started üç∏üçπ.","title":"Training a Drink Classifier with Fastai"},{"content":"","permalink":"http://localhost:1313/posts/roboflow-databricks/","summary":"","title":"Import Images from Databricks to Roboflow"},{"content":"Today I thought it would be a cool idea to link some of the blog posts I\u0026rsquo;ve written at work to my personal blog. The idea here is that I would have a hugo post that shows up in my archive, but when clicked, will redirect to where the post is hosted. Essentially, a simple external redirect. This proved to be a little more challenging with Hugo than I originally thought, hence the blog üî•.\nHugo Aliases Hugo does have a way to facilitate internal page redirects with Aliases, but I wasn\u0026rsquo;t able to apply this configuration to take a page to a completely new webpage. Along with this, as I tested the feature, I wasn\u0026rsquo;t able to get it to work. I reported a bug with my current theme I used called paper mod. More to come on this, but I didn\u0026rsquo;t do any more digging for now since I don\u0026rsquo;t need internal redirects at the moment.\nExternal Redirects External redirects require a little more advanced configuration. Huge shout out to a few resources including Dan North\u0026rsquo;s blog for some hints on configuring this setup. Here\u0026rsquo;s how it works.\nWe\u0026rsquo;ll use an internal hugo template called alias to customize the configuration. Some other embedded templates can be found here, but we\u0026rsquo;ll use the alias template. Some documentation around customizing URL management is tucked away in the documentation as well.\nLet\u0026rsquo;s take a look at the internal alias.html template below.\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ site.Language.LanguageCode }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Permalink }}\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; content=\u0026#34;noindex\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;refresh\u0026#34; content=\u0026#34;0; url={{ .Permalink }}\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;/html\u0026gt; Looking at the file, we can see there is a Permalink variable. We can set this variable in our page entry the following meta line will handle our redirect \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; url={{ .Permalink }}\u0026quot;\u0026gt;. Let\u0026rsquo;s do this. First, let\u0026rsquo;s create a template at layouts/external-redirect/single.html.\nIn this file we\u0026rsquo;ll replace the Permalink context with our own link. We can do this with the following snippet\n{{- template \u0026#34;_internal/alias.html\u0026#34; (dict \u0026#34;Permalink\u0026#34; .Params.url) -}} This will pull in the hugo alias embedded template and replace the Permalink context with a variable called url which we can pass in. Now, let\u0026rsquo;s string up our redirect. Let\u0026rsquo;s create a new post for one more my work blogs called \u0026ldquo;Making Megadesk Smart\u0026rdquo;.\n--- type: \u0026#34;external-redirect\u0026#34; url: \u0026#34;https://blog.roboflow.com/security-camera-monitoring/\u0026#34; title: \u0026#34;Making Megadesk Smart\u0026#34; date: \u0026#34;2024-03-17\u0026#34; --- And just like that, I have a new post that redirects users to another site.\nHappy Building!\n","permalink":"http://localhost:1313/posts/hugo-external-redirects/","summary":"Today I thought it would be a cool idea to link some of the blog posts I\u0026rsquo;ve written at work to my personal blog. The idea here is that I would have a hugo post that shows up in my archive, but when clicked, will redirect to where the post is hosted. Essentially, a simple external redirect. This proved to be a little more challenging with Hugo than I originally thought, hence the blog üî•.","title":"External Redirects with Hugo"},{"content":"","permalink":"http://localhost:1313/posts/smart-megadesk/","summary":"","title":"Making Megadesk Smart"},{"content":"A few weeks ago a group of coworkers went skiing. Since Jess and I had never put skiis on before, we thought it\u0026rsquo;d be a good idea to get group lessons. As the squad walked up to Breckenridge Ski Resort Peak 8 I was excited and nervous. Breckenridge is no joke. Base elevation is already a whooping 9,600 ft with a 13,000 summit elevation. The resort boast 187 trails ranging in difficulty and thousands of skiers descend the slopes daily. It\u0026rsquo;s an incredible sight to see some of the trails while walking through the town.\nHeaded to the mountain Day One Instructor After making it to the Peak 8 base, we met up with the instructor and a group of rookie skiers. The instructor sized everyone one up with a few questions about experience, then we swiftly headed out to the bunny slopes learning area for a run down. On day one, the instructor taught us some basics. We learned about posture, how to control speed, and how to turn. To be honest, it didn\u0026rsquo;t come naturally for me, and the instructor would tell me \u0026ldquo;good job\u0026rdquo; even if I clearly wasn\u0026rsquo;t getting it. After lunch the instructor viewed our progress and made a call to take a few of us up the lift to a more advanced training zone. Looking back, I wasn\u0026rsquo;t ready for this yet, and I fell 3-4 times skiing down to the practice area. After a few hours of practice, I wasn\u0026rsquo;t really improving. It was a frustrating experience. I finally asked the instructor, \u0026ldquo;What am I doing wrong?, How can I improve?\u0026rdquo;. I was met with reassurance that I was doing great and that skiing \u0026ldquo;takes a little more practice\u0026rdquo;.\nDay Two Instructor Day two, Jess and I got another group lesson to continue to learn and practice. I wasn\u0026rsquo;t looking forward to falling more, and I questioned if skiing was the sport for me. When we arrived, we hit the jackpot. We were the only two people there for lessons, which meant we essentially scored a private lesson with an experienced skier named Ron. Ron was a short and skinny old mountain man. As we headed up the mountain I noticed something different about Ron. He kept saying \u0026ldquo;I always see people skiing the wrong way, we\u0026rsquo;re going to learn the right way to ski Today\u0026rdquo;. I felt hope. We got up to the training area and just like the day before, I fell. Instantly, Ron pointed out exactly what I did wrong. \u0026ldquo;Don\u0026rsquo;t lean back like that\u0026rdquo;, he yelled. It was blunt, and he was right. At first, I felt embarrassed, and I even felt on guard. He was right though. We continued on, and this time I fell again. Ron yelled \u0026ldquo;Why do you keep leaning back? Lean forward, use the skis as a tool\u0026rdquo;. He continued pointing out each part of my skiing that needed to change. It wasn\u0026rsquo;t personal, it just wasn\u0026rsquo;t \u0026ldquo;the right way to ski\u0026rdquo;.\nLessons from Ron I took some notes on what Ron was saying that day\nThe skiis are a tool, use the wedge (pizza). Weight on the balls of the feet behind the toes. Shins should be tight against front of the boot. Hips should be forward at all times. To turn left shift weight to right side ball of foot. To turn right shift weight to left side ball of foot. Lean forward, center of gravity should be in the front of ski. Don‚Äôt squat, look down, or panic. Wear one pair of wool socks to reduce shin pain. After getting yelled at by Ron for a few hours he gave me my first \u0026ldquo;Yes! do you feel that? feels good doesn\u0026rsquo;t it?\u0026rdquo;. It did feel good! It felt natural, I was skiing! He deemed us ready to head down the green slope, and I made it down without falling.\nShredding down the mountain My Realization I did a little thinking on the flight home about this experience. I realized that without the blunt feedback I received from Ron, I wouldn\u0026rsquo;t have enjoyed day two, and I probably wouldn\u0026rsquo;t have wanted to ski again. Although Ron called me out, he was right, and it lead me to change my behavior. I hope to receive more blunt feedback in the future, so I can get better and learn new things. Thanks Ron, I really appreciate it.\nSquad on the mountain ","permalink":"http://localhost:1313/posts/blunt-feedback/","summary":"A few weeks ago a group of coworkers went skiing. Since Jess and I had never put skiis on before, we thought it\u0026rsquo;d be a good idea to get group lessons. As the squad walked up to Breckenridge Ski Resort Peak 8 I was excited and nervous. Breckenridge is no joke. Base elevation is already a whooping 9,600 ft with a 13,000 summit elevation. The resort boast 187 trails ranging in difficulty and thousands of skiers descend the slopes daily.","title":"Blunt Feedback"},{"content":"Traveling is one of my favorite hobbies. It\u0026rsquo;s an entry point to experiencing other interests of mine including trying new restaurants, enjoying entertainment, and sight seeing. It\u0026rsquo;s also a chance to learn how to explore and operate in a new city. A few weeks ago I visited New York City for the first time. I was starting a new job and onboarding took place in the heart of Manhattan. After the week of onboarding I had a full day to experience a sliver of what downtown New York had to offer.\nGame Plan over Brunch I started the day with a delicious brunch at Friedman\u0026rsquo;s. There was a huge line and an hour wait, but since I was riding solo they sat me in 5 minutes. Score! It was tough to choose from their brunch menu but I ended up fueling up with some steak \u0026amp; eggs with a cold brew. While eating I pulled out my notes app and started to plot out my travel game plan to see some of the places on my list.\nThe Game Plan:\nThe Empire State Building The 9/11 Memorial The Statue of Liberty Times Square Broadway Public Transportation At first, trying to navigate New York subways seemed overwhelming. The winding rows of trains seemed to lead everywhere and nowhere all at once. It was a bit overwhelming the first time, but Google Maps came to the rescue.\nMap of NY Subways Prior to traveling to New York, I always thought that Apple Maps was the premier navigation experience. Boy was I wrong! Google Maps crushed the public transportation game. The app knew everything from which side of the street to enter, while also maintaining real time schedule changes. It saved me a ton of stress.\nThe Empire State Building After finishing brunch and mapping out my sight seeing game plan, I walked the 10 minutes to my first stop. The Empire State Building stood tall and mighty before me. I decided to purchase the 102nd observation deck ticket, but in all honestly the 86th floor was equally as impressive. Standing on the NYC streets, it\u0026rsquo;s hard to grok how large the concrete jungle is, but from the 102nd floor it was impossible to ignore the impressive creation. If you squint just right, you can see The Statue of Liberty in the top right corner.\n102nd floor of The Empire State Building 9/11 Memorial My next stop was a sobering one. I took the subway 10 minutes in the south direction and quickly reached my destination: the 9/11 Memorial. On September 11th, 2001, 19 terrorists carried out four coordinated suicide attacks on the United States resulting in the death of 2,977 people. At 8:46 AM EST American Airlines Flight 77 crashed into the North Tower of the World Trade Center leading to the eventual collapse of the building. At 9:03 AM EST, United Airlines Flight 175 crashed into the South Tower of the World Trade Center leading to the collapse of the building. The memorial consists of the footprints of the original Twin Towers and are called the North Pool and South Pool.\nThe South Pool - 9/11 Memorial I place this video here as a reminder of the chaos and loss of that day. It contains graphic content so viewer discretion is advised.\nAfter paying my respects and pondering the events of 9/11, it was time to move to my next destination.\nThe Statue of Liberty Moving further south, my next stop was The Statue of Liberty. I decided to save an up close look for a future day but the view from the lookout was incredible. I reflected on what it must have been like to see the statue whilst coming to the United States through Ellis Island to start a new life. There are many tours that you can take to get an up close look including the free Staten Island Ferry that runs every 15 minutes during rush hour.\nThe Statue of Liberty Wall Street Wall Street was a quick walk from the Statue of Liberty and I had a goal of seeing the Charging Bull. Sometimes referred to as the Bull of Wall Street, it stands proud at over 7000 lbs and is made entirely of bronze. The statue was created by Arturo Di Modica around the 1987 Black Monday stock market crash and was originally placed outside of the NY Stock Exchange illegally. Thousands of people visit the sculpture every day.\nThe Charging Bull Times Square and Broadway I decided to head back to the hotel to rest and regroup as my next stop was Times Square to see a show on Broadway. I didn\u0026rsquo;t realize that Broadway had dozens of shows to choose from so this alone was a difficult task. After a tough decision, I thought it would be incredible to see Hamilton by Lin-Manuel Miranda on Broadway at the Richard Rogers Theatre. It was amazing.\nBroadway New York New York was an impressive and entertaining place to explore. I know I only scratched the surface of the restaurants, entertainment, and sightseeing that NY has to offer. I\u0026rsquo;m looking forward to continue exploring on my next visit. Until next time NY!\nBonus\u0026hellip; Don\u0026rsquo;t sit here ","permalink":"http://localhost:1313/posts/day-in-ny/","summary":"Traveling is one of my favorite hobbies. It\u0026rsquo;s an entry point to experiencing other interests of mine including trying new restaurants, enjoying entertainment, and sight seeing. It\u0026rsquo;s also a chance to learn how to explore and operate in a new city. A few weeks ago I visited New York City for the first time. I was starting a new job and onboarding took place in the heart of Manhattan. After the week of onboarding I had a full day to experience a sliver of what downtown New York had to offer.","title":"A Day in New York"},{"content":"In early February I wrote a post called Kauai Hawaii - Part One: Adventures. Megan and I had just returned from an amazing six day adventure to Kauai, Hawaii and in that post I talked about some of the cool things we experienced. As I was writing that post I realized that it would have been double the length if I had included all of the amazing food we tried. I quickly concluded that I would need to write a new post devoted solely to the food we experienced. Here is my attempt at reliving the meals we enjoyed the most.\nKauai Food In Kauai, there are a diverse set of restaurants ranging in style, price, and ambiance. There wasn\u0026rsquo;t a restaurant that we didn\u0026rsquo;t enjoy but some stood out as our favorites. When I say that writing this post was very difficult, I mean it. It even caused a little writer\u0026rsquo;s block as I didn\u0026rsquo;t want to decide on my favorite spots for breakfast, lunch, snacks, and dinner. With that being said, here is how I\u0026rsquo;m currently feeling üôà.\nFavorite Breakfast Spot In my opinion, choosing the Tip Top Motel, Cafe, and Bakery as the number one breakfast spot was the easiest choice. This spot has been around for 107 years (as of writing this blog post) and was originally founded by a plantation manager named Denjiro Ota in 1916! It\u0026rsquo;s an unassuming hole-in-the-wall attached to a motel and has both charm and character. I shit you not, this place had two of my favorite dishes that I had on the entire trip.\nThe first is their homemade banana macnut pancakes. This recipe was created by the son of Denjiro named Mitchell after he took over the business in 1925. To this day, they still serve the same recipe.\nFamous Pancake Recipe The second is their oxetail soup. This dish had a incredible mix of flavors. The oxtail was tender, fresh, and delicious.\nOxtail Soup Favorite Lunch Spot Our favorite lunch spot had a little more competition than our favorite breakfast spot. The Koloa Fish Market came out ahead by a thread. This spot boasts the best poke bowls I\u0026rsquo;ve had in my life. Founded by the Matsuoka family 25 years ago right after Hurricane Iniki, you can tell they really care about quality. The location is off of Poipu Road and is a simple walk-up counter. They have a great selection of ahi tuna, salmon, marlin, and shrimp poke. Below is my ahi tuna poke bowl with edamame, ginger, ocean salad, and cucumber. I\u0026rsquo;d go back in a heartbeat.\nAmazing Ahi Poke Favorite Snack Spot When we weren\u0026rsquo;t eating meals like breakfast and dinner, we were eating snacks! Kauai Kookie was my favorite go-to snack on the island. Founded in 1965 by Mabel Hashisaka with humble begginings, the business now bakes around 50,000 cookies and specialty items every single day! That\u0026rsquo;s a lot of goodies\u0026hellip; My personal favorite was the OG Chocolate Chip Macadamia and the Kona Coffee Shortbread!\nKauai Kookie Varieties Favorite Dinner Spot The last on our list of favorites was Tidepools at the Grand Hyatt Resort. To start, this resort was insanely huge and impressive. It reminded me a lot of a Las Vegas resort where you have everything you\u0026rsquo;d ever need for an incredible vacation all on the resort grounds. Although the resturant was booked out for months, our concierge called in a favor and got us a reservation for two for my birthday dinner ü•∞. The resturant sits on a floating structure above a koi-filled lagoon at the base of a waterfall. They had an incredible menu with fresh fish and steak.\nThe Hawaiian Catch The Prime Center Cut Filet Honorable Mentions Of course, I\u0026rsquo;d be crazy not to bring up all the other amazing food we were able to experience. Here is an expansive list of all of the places we tried.\nKalaheo Cafe \u0026amp; Coffee Co Hamura Saimin Red Salt at Ko\u0026rsquo;a Kea Holey Grail Donuts L\u0026amp;L Hawaiian Barbecue JoJo\u0026rsquo;s Shave Ice Tahiti Nui Pat\u0026rsquo;s Taqueria Keoki\u0026rsquo;s Paradise Puka Dog Hawaiian Style Hot Dogs Da Crack Mexican Grinds JO2 Natural Cuisine Eden Coffee Kenji Burger Kiawe Roots Duke\u0026rsquo;s Kauai Until next time Kauai! ü§ô\n","permalink":"http://localhost:1313/posts/kauai-hawaii-food/","summary":"In early February I wrote a post called Kauai Hawaii - Part One: Adventures. Megan and I had just returned from an amazing six day adventure to Kauai, Hawaii and in that post I talked about some of the cool things we experienced. As I was writing that post I realized that it would have been double the length if I had included all of the amazing food we tried. I quickly concluded that I would need to write a new post devoted solely to the food we experienced.","title":"Kauai Hawaii - Part Two: Food"},{"content":"Some of you that read my blogs know that I\u0026rsquo;m a pretty big fan of Rocket League. For those of you that aren\u0026rsquo;t aware of this game, please allow me to change your life. The best way to describe Rocket League is that it\u0026rsquo;s half soccer half demolition derby. You play as a flying car and do your best to kick a soccer ball into your opponents\u0026rsquo; net. For those who still aren\u0026rsquo;t really getting what Rocket League is, look at this epic goal below.\nThe only way I play the game is competitively via 1v1, 2v2, or 3v3 game play. Now just because I only play competitively doesn\u0026rsquo;t mean I play well, but I digress. The last few times I\u0026rsquo;ve played the game, there was some pretty gnarly lag. This lag made competitve game play pretty frusturating. It was happening across multiple weeks so I was fairly certain it had to do with my own internet connection. Part of me wondered if it was due to the placement of my wifi access point or the fact that I was gaming over a wifi connection to begin with. Some of you might ask:\n\u0026ldquo;Why not just run ethernet to the PS5 and call it good\u0026rdquo;. - Some of you\nThis is a great idea and my initial thought! The problem, though, is that I live in a three-story town home, of which my main networking gear comes in on the first floor, and my PS5 sits in the basement. I also rent, which means that I wasn\u0026rsquo;t about to drill holes in the walls/floors to run ethernet to the basement.\nEnter Powerline I was feeling a bit hopeless, when I happened to be listening to this episode from one of my favorite IT podcasts called 2.5 Admins. At minute marker 20:38, a listener had the same problems that I had around not being able to run ethernet and was wondering if powerline might be a good option. Instantly I questioned\u0026hellip; \u0026ldquo;what the heck is powerline?!\u0026rdquo;. Well, after some digging, powerline seems like it should be illegal or magical. The steps to setting this up are below:\nBuy a set of powerline adapters. Plug one into to an outlet near your router and connect ethernet to it. Plug the other into an outlet near a device needing internet and connect ethernet to it. Packets are sent over your electrical circuit. After learning about this, I purchased a pair of tplink powerline adapters and got them all hooked up! Below are some speed test results.\nPS5 Internet Testing Results For my first test, I thought I\u0026rsquo;d go straight to the source and run a PS5 internet speed test. The numbers are pretty simple but still gave some valuable insight.\nConnection Upload Speed (Mbps) Download Speed (Mbps) 5 GHz Wifi 81.6 9.6 Powerline 101.4 14.1 So both my upload and download speeds were better on the PS5. Pretty cool.\nI then thought it\u0026rsquo;d be cool to run a test through Cloudflare\u0026rsquo;s speed test web page. It was a little challenging finding a way to access a browser on the PS5, but it\u0026rsquo;s possible if you got to \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Users and Accounts\u0026rdquo; \u0026gt; \u0026ldquo;Link Account\u0026rdquo; \u0026gt; \u0026ldquo;YouTube\u0026rdquo;. From youTube, you can make your way to the Google search bar to get to https://speed.cloudflare.com/.\nCloudflare Speed Test Results Cloudflare has a cool Network Quality Score which takes into account latency, packet loss, download, upload, loaded latency, and jitter. From there it calculates a network score of bad, poor, average, good, and great.\nWithout diving into the numbers, I\u0026rsquo;ll give you the results in term of the bad-great rates for each category.\nConnection Video Streaming Online Gaming Video Chatting 5 GHz Wifi Good Average Average Powerline Good Great Great This is relatively new change so I\u0026rsquo;ll see how the actual feel of game play is improved over time, but I\u0026rsquo;m pretty optomistic that powerline will have solved my issue. More to come on if this is the case! Cheers!\n","permalink":"http://localhost:1313/posts/improved-gaming-powerline/","summary":"Some of you that read my blogs know that I\u0026rsquo;m a pretty big fan of Rocket League. For those of you that aren\u0026rsquo;t aware of this game, please allow me to change your life. The best way to describe Rocket League is that it\u0026rsquo;s half soccer half demolition derby. You play as a flying car and do your best to kick a soccer ball into your opponents\u0026rsquo; net. For those who still aren\u0026rsquo;t really getting what Rocket League is, look at this epic goal below.","title":"Improved Gaming with Powerline"},{"content":"I was in the process of cloning the code for this website from github on a new laptop when I noticed something.\ngit clone git@github.com:NickHerrig/nickherrig.com.git The repo was taking a noticable amount of time to clone. I timed how long it took and found that it was about 8.5 seconds!\nCloning into \u0026#39;nickherrig.com\u0026#39;... remote: Enumerating objects: 583, done. remote: Counting objects: 100% (76/76), done. remote: Compressing objects: 100% (50/50), done. remote: Total 583 (delta 23), reused 72 (delta 22), pack-reused 507 Receiving objects: 100% (583/583), 78.74 MiB | 11.49 MiB/s, done. Resolving deltas: 100% (226/226), done. git clone git@github.com:NickHerrig/nickherrig.com.git 1.08s user 1.94s system 35% cpu 8.524 total I had a gut feel it was related to the images that I\u0026rsquo;m storing in the repo, but I wanted to make sure. To find out I ran a du -sh * to try and find the largest directory.\ndu -sh * 4.0K README.md 4.0K archetypes 88K content 4.0K hugo.yaml 74M static 0B themes From here I noticed that my static directory was 74M. Gut feel confirmed. This is where I store all my blog images. A little more digging showed that I was storing 26M of images from my most recent blog post Kauai Hawaii - Part One: Adventures. As I look forward to writing more blog posts with image content, I realized this doesn\u0026rsquo;t really scale well with the usability of my git repo. I started doing a little research and decided to tackle this problem in two ways.\nImage Resizing and Formating When I realized my images were pretty large, I wanted to confirm this with another form of analysis. I reached for a popular tool called PageSpeed Insights. You can use this tool to run an analysis on specific pages of a website to get a report on performance issues.\npagespeed results for kauai adventures blog One call out was to utilize next-gen formats like webp. Another call out was to properly size the images. I decided to write a quick shell script to change the form of my images fro jpg to webp and reduce the quality.\nfor img in *.jpg; do magick convert \u0026#34;$img\u0026#34; -quality 80 \u0026#34;${img%.jpg}.webp\u0026#34; done With that, we\u0026rsquo;re down to 11M from 26M! Not bad! Next, I wanted to resize the images. In order to do this, I needed to first identify the current size of the images, then figure out what the computed values were on the website. To run this example, let\u0026rsquo;s look at the caves image.\nidentify -format \u0026#34;%wx%h\\n\u0026#34; caves.webp 4032x3024 When looking at the deployed production site, the same image height was 540px and width was 720px. This means we can resize to the computed value, and maybe even get away with bumping the quality to 100%. Let\u0026rsquo;s try it out.\ndu -sh caves.webp 1.8M caves.webp magick caves.webp -resize 720x540 -quality 100 _caves.webp du -sh caves.webp _caves.webp 1.8M caves.webp 496K _caves.webp And just lke that we\u0026rsquo;ve saved a ton of space, and my human eyes cannot tell the difference! I\u0026rsquo;ll spare you the repetitveness of doing this to every image in my repo for the purpose of this blog.\nThe Numbers Now when I take a peak at my static directory here\u0026rsquo;s what I see.\ndu -sh * 4.0K README.md 4.0K archetypes 84K content 4.0K hugo.yaml 4.1M static # Used to be 74M 792K themes Going from 74M to 4.1M with literally zero implication to noticable image quality is pretty great!\nLet\u0026rsquo;s take a look now at how long it takes to do a git pull.\nCloning into \u0026#39;nickherrig.com\u0026#39;... remote: Enumerating objects: 720, done. remote: Counting objects: 100% (213/213), done. remote: Compressing objects: 100% (145/145), done. remote: Total 720 (delta 60), reused 202 (delta 54), pack-reused 507 Receiving objects: 100% (720/720), 97.09 MiB | 9.78 MiB/s, done. Resolving deltas: 100% (263/263), done. git clone git@github.com:NickHerrig/nickherrig.com.git 1.29s user 2.23s system 29% cpu 11.739 total Shit, it appears we went backwards here from 8.5 seconds to 11.8 seconds. I\u0026rsquo;m assuming this is because git stores a history of changes to the repo, so all we really did here was add a bunch of smaller images to the repository. Enter git LFS.\nGit LFS Git Large File Storage (LFS) is a tool that helps keep your repository light by storing large files separately from your main git repo. Here is a direct quote from the documentation on how it works.\nGit LFS handles large files by storing references to the file in the repository, but not the actual file itself. To work around Git\u0026rsquo;s architecture, Git LFS creates a pointer file which acts as a reference to the actual file (which is stored somewhere else). GitHub manages this pointer file in your repository. When you clone the repository down, GitHub uses the pointer file as a map to go and find the large file for you.\nIn order to get this working for my GitHub Pages hosting, it appears that with this GitHub Discussion all you need to do is add a little configuration to your github actions.\n- name: Checkout uses: actions/checkout@v4 with: lfs: true At this point, I made sure to initialize git LFS and track specific file types.\ngit lfs install git lfs track \u0026#34;*.jpg\u0026#34; git lfs track \u0026#34;*.jpeg\u0026#34; git lfs track \u0026#34;*.png\u0026#34; git lfs track \u0026#34;*.webp\u0026#34; git lfs track \u0026#34;*.mp4\u0026#34; git lfs track \u0026#34;*.mp3\u0026#34; Since git lfs only starts tracking newly added files, we\u0026rsquo;ll want to migrate our existing files. We can do this by running.\ngit lfs migrate import --include=\u0026#34;*.jpg,*.jpeg,*.png,*.webp,*.mp4,*.mp3\u0026#34; --everything git push --force And just like that, we\u0026rsquo;ve improved the speed of our repo clones from 11.8 seconds to 4 seconds.\nCloning into \u0026#39;nickherrig.com\u0026#39;... remote: Enumerating objects: 867, done. remote: Counting objects: 100% (397/397), done. remote: Compressing objects: 100% (237/237), done. remote: Total 867 (delta 129), reused 393 (delta 127), pack-reused 470 Receiving objects: 100% (867/867), 10.11 MiB | 10.48 MiB/s, done. Resolving deltas: 100% (316/316), done. Filtering content: 100% (36/36), 3.08 MiB | 1.85 MiB/s, done. git clone git@github.com:NickHerrig/nickherrig.com.git 0.34s user 0.50s system 20% cpu 4.076 total Cheers to Saturday optimizations! üç∫\n","permalink":"http://localhost:1313/posts/fixing-slow-git/","summary":"I was in the process of cloning the code for this website from github on a new laptop when I noticed something.\ngit clone git@github.com:NickHerrig/nickherrig.com.git The repo was taking a noticable amount of time to clone. I timed how long it took and found that it was about 8.5 seconds!\nCloning into \u0026#39;nickherrig.com\u0026#39;... remote: Enumerating objects: 583, done. remote: Counting objects: 100% (76/76), done. remote: Compressing objects: 100% (50/50), done. remote: Total 583 (delta 23), reused 72 (delta 22), pack-reused 507 Receiving objects: 100% (583/583), 78.","title":"Solving Slow Git Repos"},{"content":"My wife Megan and I just returned from a 6 day adventure to Kauai, Hawaii. It was incredible. There was so much to see, eat, and do that it was physically impossible to experience the full scope of what the island has to offer during our short visit. In an effort to remind myself of our stay, I thought it\u0026rsquo;d be a fun exercise to try and stack rank a few of the adventures we shared while exploring the garden island. It turned out to be harder than I thought üôà!\nThe Garden Island Kauai, Hawaii is the oldest of the main Hawaiian island chain: around 5 Million years old ü§Ø. It\u0026rsquo;s also one of the smallest islands coming in at 33 miles wide and 25 miles long. Unlike some of Kauai\u0026rsquo;s larger neighbors, there are no active volcanos. Mt Wai ªale ªale poured its final drop of molten lava around 500,000 years ago. The garden island gets its name from the incredibly lush rainforests while also boasting rugged canyons and mountainous north shores.\nThe Geography Prior to the trip, Megan and I scouted out the island and came up with a loose game plan to help us efficiently explore (must be the Industrial Engineers in us üòÇ). We quickly discovered that it was a sound strategy to split the island into 4 quadrants: north, sourth, east, and west. One thing to note is that due to the rugged landscape, roads do not connect the west and north sides of the island. This, paired with the fact that the main highway system runs parallel to the coast, means that if you want to travel from west to north, you\u0026rsquo;ve got a hell of a drive ahead. Speaking of driving, one thing I\u0026rsquo;d strongly recommend is renting a vehicle for your trip. Without a vehicle, exploring the island activities and food scene would have been challenging.\nThe Resort Although Kauai has a solid vacation rental scene, we opted for the slightly more expensive resort route. Staying at a resort meant that we were right on the beach and had access to resort amenaties like a pool, hot tub, restaurant, and a clean room to come home to. We stayed at Ko\u0026rsquo;a Kea Resort on the south side of the island in Poipu. When we return to Kauai, we might look into doing an Airbnb but we loved our stay at Ko\u0026rsquo;a Kea.\nOur Favorite Adventures Although it\u0026rsquo;s impossible to narrow it down, below are my favorite two adventures that we did. Since Megan and I love trying new food and restuarants on trips, I kept all mention of food in Part Two (coming soon\u0026hellip;) of this blog series. Let\u0026rsquo;s get on with the adventures.\nHelicopter Ride Every blog, YouTube video, and conversation about Hawaii mentioned a helicopter ride. To be honest, I was a little skeptical. I debated the price tag. How could it be worth the money? Helicopter rides run a beefy price tag of around $400-$450 per seat. I humbly admit that I could not have been more wrong. It was worth every penny. We booked our helicopter tour with Blue Hawaiian as we wanted a trust worthly, \u0026ldquo;door on\u0026rdquo; experience. \u0026ldquo;Door on??\u0026rdquo; you might ask. Yes, you read that correctly. For the more adventurous and daring, you can opt for a helicopter ride with no doors. Our tour left from the Lihue airport located on the East side of the island.\nOur Blue Hawaiian Helicopter The tour took us up the west coast, first over the port Hawaii where cruise ships make their home. From there we ascended on old sugar cane fields owned by the Robinson family, a prominent land and island owning family. As we looked to our left, Niihau, also known as the Forbidden Island, could be seen in the distance. This island was purchased by the Robinson family in 1864 for 10,000 gold and a grand piano. In today\u0026rsquo;s dollars, that\u0026rsquo;s only $300,000; not a bad deal for an island. Another interesting fact about the island, is that only natives are allowed: no tourists (hence the name forbidden).\nMountains and Fields in West Kauai From there, we headed towards Waimea Canyon. At this point, my jaw dropped and remained dropped for the duration of the flight. The advantage of a helicopter is that you get an up close and personal view flying through the canyon. Waterfalls, lush green, and bright red iron soil engulfed the aircraft.\nFlying Past Waimea Waterfalls I questioned how it could get better than this. At that moment we headed out of the Canyon and in the direction of the infamous Napali Coast. It was humbling, inspiring, and breathtaking. Something about a landscape being formed and molded over millions of years puts your own existence into perspective.\nThe Beautiful Napali Coast From here, we headed back to the airport. The ride was about 50 minutes total. If you get a chance to visit Kauai, do not leave this adventure off of your list.\nNapali Coast Boat Ride After seeing the Napali Coast from the air, we opted for a closer look at the coast line via a boat ride and snorkling adventure. There are a few great companies to choose from, but we ended up hanging out with Captain J\u0026rsquo;s. The trip started with an early sighting of a pod of more than 20 bottlenose dolphins. The Captain said this was rare to see so many feeding at the same time so we were grateful to have stumbled upon them. While we ventured on, we made our way up west side of Kauai towards the Napali coastline. We were quickly greeted by impressive cave formations.\nNapali Coast Caves The coastline is hard to put into words without seeing it yourself. It\u0026rsquo;s a 17 mile stretch of volcanic rocks, stunning cliffs, and lush greenery. It\u0026rsquo;s inaccesible by road, however, but can be accessed by the Kalalau Trail. Sadly, due to weather, we weren\u0026rsquo;t able to do this trail. The river was too high due to higher than normal rainfall during our stay. The boat ride allowed us to get a different, less exhausting view.\nBoat Views of the Napali Coast On our way to the snorkling spot, we had another encounter with wildlife. This time, we spotted a group of humpback whales. The whales were showing their tails and fins, most likely in an attempt to impress the larger female of the group. It was a magical moment seeing these large creatures. We finally made our way to our snorkling spot. Although Hawaii isn\u0026rsquo;t known for coral reefs, the rock formations were filled with bright colored fish including a red spotted octopus that Megan spotted in the water.\nHonorable Mentions Okay, I lied. I wasn\u0026rsquo;t able to narrow it down to just two of my favorites. Here are a few other items that you shouldn\u0026rsquo;t miss!\nA walk on Poipu Beach Exploring Waimea Canyons Visiting Wailua Falls Resting at the Hanalei Valley Lookout Swimming at Hanalei Beach Watching surfers at Ha\u0026rsquo;ena Beach Exploring Tunnels Beach The Kilauea Lighthouse A visit to Spouting Horn Relaxing by the pool / hot tub Turtles Resting on Poipu Beach Until next time Hawaii! Mahalo! ü§ô\n","permalink":"http://localhost:1313/posts/kauai-hawaii-adventures/","summary":"My wife Megan and I just returned from a 6 day adventure to Kauai, Hawaii. It was incredible. There was so much to see, eat, and do that it was physically impossible to experience the full scope of what the island has to offer during our short visit. In an effort to remind myself of our stay, I thought it\u0026rsquo;d be a fun exercise to try and stack rank a few of the adventures we shared while exploring the garden island.","title":"Kauai Hawaii - Part One: Adventures"},{"content":"For as long as I can remember, I\u0026rsquo;ve never been particularly proud or excited about my desk setup. My current setup is a cheap 25\u0026quot; x 50\u0026quot; IKEA desk that is neither personalized nor very functional. It\u0026rsquo;s made of a cheap-feeling cardboard-like material, lacks adequate space, and causes frustration from time to time.\nAs I was thinking about these problems, I remembered an epic scene from Ricky Gervais and Stephen Merchant\u0026rsquo;s American adaptation of \u0026ldquo;The Office\u0026rdquo;. Jim and Dwight were battling out who could build the most epic desk. Mega desk and quad desk were true masterpieces of commedy.\nTo quote Dwight on the feeling he had when using mega desk:\n\u0026ldquo;All I care about is megadesk, that is all I care about; getting\u0026hellip; more\u0026hellip; megadesk\u0026rdquo;\nIf Dwight was so passionate about mega desk, and Jim went on to build a mega desk of his own to thwart Dwight, maybe there was something to this whole mega desk idea. It was time for me to build my very own mega desk.\nBrainstorming When setting out to build mega desk, I didn\u0026rsquo;t really know where to start. The options seemed endless. What color? Standing or sitting? Do you go with an L or U shape? What about drawers? To gain some traction, I thought it\u0026rsquo;d be a good experiment to walk through some of reasons I wasn\u0026rsquo;t a fan of my existing setup.\nThe desk felt cheap. There was never enough space for projects. Sitting for a while was uncomfortable. The color didn\u0026rsquo;t match the space. So for this project to be a success I would need to focus on fixing these pain points. I would need to set out to:\nUse better material (maybe a hardwood or metalic material). Build a bigger desk. Incorporate a desk frame with a standing option. Use material or paint to match the space. With this brainstorm session in mind, I wondered what other people who built their own desk had experienced. Luckily, I\u0026rsquo;ve got a great tech friend group that is always willing to jam on ideas. I posed a simple question on our Discord server.\nAdvice came in hot! A common thread was to utilize butcher block as the top and to purchase a lift frame to pair with the hardwood. With that information, I was off to the races!\nThe Game Plan With a solid collection of research under my belt, it was time to craft the build of materials. I needed to source a standing desk frame in an L shape, two butcher block slabs of various length for the top, and some stain/finish to coat the material.\nA Note on Impatience The most challenging part of the build was sourcing the materials to meet the deadline. I ordered the standing desk frame from Amazon and, once it arrived, I had 2 days remaining to make the desk a reality. Sourcing the perfect sized butcher block in that time frame turned out to be more challenging than I originally suspected. Lowe\u0026rsquo;s, Home Depot, and IKEA all have great selections. There were also a few local places in Des Moines that also had a decent variety. From my measurements of my existing space, the perfect size desk would be comprised of two 25\u0026quot; width by 60\u0026quot; length butcher block slabs. Sadly, I would have had to wait another week for this material to be delivered. Instead, I settled for taking the impatient route. This meant purchasing the 25\u0026quot; width by 72\u0026quot; long butcher block slabs. This made total length of the main desk 25 + 72 = 97 inches long: MEGA DESK!\nThe Materials Here is the list of the final BOM I purchased:\nA Vivo L shaped standing desk frame A couple 72\u0026quot;L x 25\u0026quot;W x 1.5\u0026quot;T Aspen + Roth Hevea Butcher Block slabs Oil-based wood stain Oil-based finish Foam paint brushes Execution Once I had the list of materials the build was suprisingly straightforward. The standing desk came with super clear assembly instructions, including going about drilling pilot holes for screws, how/where to add brace plates, and how to lay out the electrical components.\nI followed the following loose steps:\nGet the butcher block into the correct possition. This meant aligning the desk upside down and inverted from it\u0026rsquo;s final location. This step was pretty important as the desk is pretty heavy, especially after mounting the frame. The next step was positioning the standing desk frame on top of the desk. There was some eyeballing on the best place for the extension leg, but I used a tape measure to ensure it was aligned in the center. After the frame was in place, it was time to drill some pilot holes. I used a 1/8\u0026quot; drill bit for this job. A pro tip was to plug in all the electronic components prior to drilling pilot holes to ensure the cabeling could reach prior to mounting. With the pilot holes drilled, it was time to screw in the frame and the electronic components into the desk. This step also included mounting the silver brace plates where the two pieces of butcher block meet for extra support. Flipping the desk over and into position was by far the hardest part. For a desk this size it required a minimum of two people. Luckily my wifey Megan came in clutch here! Staining and finishing the desk came next. I only applied one coat of stain and one coat of finish with a few hours in-between for the stain to dry. Note to self: crack a window next time to avoid your room smelling like chemicals. For future builds, staining and finishing might be the first step of the build but I was happy with this method. Side note: I did not stain and finish the bottom of the desk out of laziness (but let\u0026rsquo;s keep that between us). The Final Product Now to be honest, I\u0026rsquo;m currently traveling at 522 mph, 32,000 feet over the Pacific Ocean on a flight to Kauai, Hawaii while writing this. So with that said, I\u0026rsquo;ve yet to actually utilize the desk\u0026hellip; I haven\u0026rsquo;t setup my monitors, networking gear, or my computer vision hardware. With that being said, while sitting at the desk, I couldn\u0026rsquo;t help but feel proud of how the project turned out. The material looks and feels great. The standing desk functionality works perfectly. There is also more space than I know what to do with (although ask me in a month if this is still the case). Cheers to building new things!\n","permalink":"http://localhost:1313/posts/desk-build/","summary":"For as long as I can remember, I\u0026rsquo;ve never been particularly proud or excited about my desk setup. My current setup is a cheap 25\u0026quot; x 50\u0026quot; IKEA desk that is neither personalized nor very functional. It\u0026rsquo;s made of a cheap-feeling cardboard-like material, lacks adequate space, and causes frustration from time to time.\nAs I was thinking about these problems, I remembered an epic scene from Ricky Gervais and Stephen Merchant\u0026rsquo;s American adaptation of \u0026ldquo;The Office\u0026rdquo;.","title":"Building Mega Desk"},{"content":"I\u0026rsquo;ve been fishing for 25 years. Fishing for me is both an adventure and a chance to disconnect. As a kid, I would fish local honey holes with texas rigs for large mouth bass. On family vacations to Florida, I\u0026rsquo;d plan Surf fishing excursions in the Atlantic for sharks, stingrays, and anything else that would bite. Recently, I\u0026rsquo;ve had the chance to fish with the father-in-law for Walleye and Small mouth in Sioux Narrows. If there\u0026rsquo;s a fishing trip, you can count me in.\nIce fishing, on the other hand, is new territory for me. This is the second year I\u0026rsquo;ve had the opportunity to give it a go. Ice fishing is a different beast than the other styles of fishing. It\u0026rsquo;s both psychotic and exciting. You\u0026rsquo;re battling sub-zero tempatures on snow covered lakes at a chance for a good bite and even better company. So here\u0026rsquo;s a little recap from our 2024 ice fishing adventure.\nThe Location This year\u0026rsquo;s trip was in Merrifield, Minnesota, which is about 10 miles north of the larger Brainerd, MN. We stayed in an airbnb on the west shore of Sliver Lake which is marked on the map as a red circle. Our main game plan was to fish Edward Lake via vehicle access marked by a green circle. Edward Lake has a max depth of 75 feet, and is known for great structure that supports walleye, northern pike, and panfish.\nMerrifield was brutally cold this year with highs reaching low single digits and the coldest night reaching -11 degrees. The cold weather made for some thick ice, but also created a sitatuion for extreemly cold fingers and toes.\nThe Crew Nate, Wes, and Andrew ventured out for their first ice fishing adventure a few years ago. Ever since the initial cohort, the trip has seen new faces sub in and out. This year the crew was made up of 6 people. Gabe, Paul, Nate, Wes, Andrew, and myself.\nIt\u0026rsquo;s pretty cool to see how everyone brings something different to the table. People assume roles and help make the trip a success in their own unique way. Here is my take on the crew.\nNate - The Planner and Builder Nate planned a majority of the trip and always has something unique he\u0026rsquo;s built to test out on the water. This year, was a single person shanty that he built from scratch. Due to some truck difficulties, the shanty was never put to use, but his passion for planning and his building skills are always appreciated on the trip. Nate caught a sweet northern pike on Sliver Lake the first day. It\u0026rsquo;s good luck to give your fish a kiss before you release it back into the water, even if your fish has razor sharp teeth üôà.\nPaul - The Muscle Paul is one of the nicest guys you\u0026rsquo;ll meet. He\u0026rsquo;s the gentle giant of the group. Coming in at 6 foot tall, he\u0026rsquo;s the go-to for drilling stakes into the ice so our shelter doesn\u0026rsquo;t blow away in the harsh conditions. He also always jumps in to lend a helping hand with hauling gear. Paul caught a sick northern on Silver Lake the first day on the ice. Was it the same fish as Nate\u0026rsquo;s? We\u0026rsquo;ll never know üòÇ.\nGabe - Mr. Clean One thing I couldn\u0026rsquo;t help but notice on this trip (compared to past trips) was how the airbnb was kept in pretty solid shape throughout the trip. This was uncharacteristic of past trips. I started paying attention a bit more and noticed that we had Mr. Clean on the crew. Gabe was always helping to straighten up the space when myself or Nate came through like a tornado. This was a welcome addition, helping keep other aspects of the trip less chaotic. Gabe caught his first ever fish on this trip, a nice little walleye.\nWes - The Fixer One thing that is constant on an ice fishing trip is that things break and chaos ensues. On day two, Nate\u0026rsquo;s truck wouldn\u0026rsquo;t start. To make matters worse, it wouldn\u0026rsquo;t start while in the middle of the lake. Wes jumped into focus mode and started troubleshooting with Paul. They tried everything: a new battery, jump starting, hitting the starter with a wrench, starter fluid in the intake. When all hope was lost and an expensive towing bill was in sight, Wes prayed to the Fixer Gods and the truck started. Without Wes on this trip, Nate\u0026rsquo;s pickup truck might still be stranded on Edward Lake. Sadly, Wes didn\u0026rsquo;t catch any fish on this trip as he was busy enabling everyone\u0026rsquo;s success by fixing the truck.\nAndrew - The Optimizer Andrew in an OG when it comes to this ice fishing trip. Each year, he leaves the trip thinking \u0026ldquo;what could we do differently to make it less chaotic and more successful next year.\u0026rdquo; This continous improvement was seen this year through the food situation. Last year the calorie intake was mostly chips, candy, and beer. This year however; we were blessed with a delicous breakfast casserole (prepared by his wife, Monica) that gave us the energy we needed to battle the environment. I\u0026rsquo;m looking forward to seeing Andrew\u0026rsquo;s suggestions for next year. Andrew took the win on biggest fish, with this nice northern pike from Edward Lake.\nMe - The Spot Finder I tend to focus on the fishing and figuring out spots on the lake where fish are hiding out. I enjoy the process of trying to figure out a new lake I\u0026rsquo;ve never fished before. It\u0026rsquo;s like a complex puzzle with a clear result: you catch fish or you don\u0026rsquo;t. On day two this year, I attempted to find a nice spot for walleye via Edward Lake\u0026rsquo;s Topographic map. One strategy is to find shallow humps and fish the points where the water goes from shallow to deep. I found a spot that looked promising and we setup shop. There was a little dinner rush where fish came in from deeper water from 4:30 - 5:30 PM and we caught 6 fish, although all were pretty small. It was fun to pull fish out of 25 foot of water in the middle of an ice covered lake. Here\u0026rsquo;s my little walleye from that spot.\nWrapping Up Overall we had a great time, even when things didn\u0026rsquo;t go as planned. A fishing trip is more than just the fish you catch. It\u0026rsquo;s about the people you\u0026rsquo;re with and the memories you make. It\u0026rsquo;s a chance to disconnect from the hustle of every day life and focus on the task at hand with the unique skillset you bring. I look forward to continuing to improve our ice fishing adventure for many years to come. Hopefully with less truck issues next year üòÇ.\n","permalink":"http://localhost:1313/posts/icefishing/","summary":"I\u0026rsquo;ve been fishing for 25 years. Fishing for me is both an adventure and a chance to disconnect. As a kid, I would fish local honey holes with texas rigs for large mouth bass. On family vacations to Florida, I\u0026rsquo;d plan Surf fishing excursions in the Atlantic for sharks, stingrays, and anything else that would bite. Recently, I\u0026rsquo;ve had the chance to fish with the father-in-law for Walleye and Small mouth in Sioux Narrows.","title":"Ice Fishing Trip 2024"},{"content":"This year I\u0026rsquo;m starting a new role with Roboflow. I\u0026rsquo;ll be helping companies build and deploy computer vision applications. The team, the product, and the mission are all incredible and I\u0026rsquo;m excited to be a part of building the future of computer vision. With this in mind, I thought it\u0026rsquo;d be fun to document my journey as I learn some of the tools and techniques that we use to build computer vision applications \u0026ndash; starting with the Supervision python package. Here is a sneak peak at some of the things we\u0026rsquo;re going to build and cover.\nWhat is Supervision? Visiting the Supervision documentation a purple widget reading \u0026ldquo;Ask AI\u0026rdquo; caught my eye. I decided to ask it What is the Supervision python package?.\n\u0026ldquo;Supervision is an open-source Python package developed and maintained by Roboflow. It is designed to facilitate the creation of computer vision applications by providing a range of utilities. The package is distributed under an MIT license, allowing you to use it freely in your projects, whether they are public or private\u0026hellip; \u0026quot;\nAsk AI Widget The bot went on to describe what you can build with Supervision, and the various utilities available for tracking, annotation, and filtering detections. I was impressed that the project is empracing recent developments of Large Language Models (LLMs) and Retrieval Augments Generation (RAG) inside of the docs to help users quickly find the information they need to guide their project. The bot\u0026rsquo;s response serves as inspiration for some of the experiments in this post.\nWhere to start? Before we start playing with the package, let\u0026rsquo;s scaffold our project and install the package. Let\u0026rsquo;s also make sure we have a video to show off some of the features and functionality.\nInstallation The first step is to install the Supervision package. This can be done with the following pip command in a terminal of your choice. Personally, I like to first create a virtual environment so that my projects\u0026rsquo; deppendencies are isolated. Let\u0026rsquo;s install the Supervision pre-release version, as it contains some new features that we\u0026rsquo;ll be using.\npython3 -m venv venv \u0026amp;\u0026amp; source venv/bin/activate \u0026amp;\u0026amp; pip install \u0026#34;supervision==0.18.0rc1\u0026#34; Video Asset Now that we have the package installed, we need a video to test and demo. Luckily for us, the Supervision package has a great selection of high quality video assets that can be used for project ideas and demos. Let\u0026rsquo;s utilize the video of vehicles driving on a highway. You can install the video either with the python package or any other way you\u0026rsquo;d download a video from a link on the internet. I\u0026rsquo;ll be using wget to install the video in my current directory.\nwget https://media.roboflow.com/supervision/video-examples/vehicles.mp4 --output vehicles.mp4 And voila! We\u0026rsquo;ve got the perfect video for our project. Let\u0026rsquo;s take a look.\nSupervision Features Now that we\u0026rsquo;ve got Supervision installed and a video asset for our project, let\u0026rsquo;s checkout some of the features that the package has to offer. From the documentation there\u0026rsquo;s a lot to play with, including annotators, trackers, and other tools/utilities. Let\u0026rsquo;s start with detecting vehicles in the video.\nDetecting Vehicles In order to detect vehicles we\u0026rsquo;ll need a model, preferably a model that has been trained on vehicle data. Here is where Supervision features start to shine. The package inlcudes a variety of connectors for popular models from Ultralytics, Meta, and Roboflow. So with a single line of code, we can swap out the model we\u0026rsquo;re using in our application ü§Ø. A full list of connectors can be found in the documentation. Since we\u0026rsquo;re detecting vehicles, let\u0026rsquo;s use the pretrained YOLOv8 model from Ultralytics. There are a few ways we can do this, but in this project we\u0026rsquo;ll use another Roboflow package called Inference. We\u0026rsquo;ll do a deep dive on Inference in the future, but for now just know that it\u0026rsquo;s an open source package that supports running object detection, classification, instance segmentation, and foundational models that also provides a ton of advanced uses and deployment options.\nTo install Inference run the following command in your terminal.\npip install inference Now that we have Inference installed, let\u0026rsquo;s write a little bit of code.\nimport supervision as sv from inference.models.utils import get_roboflow_model if __name__ == \u0026#39;__main__\u0026#39;: # load the yolov8X object detection model from roboflow inference model = get_roboflow_model(\u0026#39;yolov8x-640\u0026#39;) # get frames iterable from video and loop over them frame_generator = sv.get_video_frames_generator(\u0026#39;vehicle.mp4\u0026#39;) for frame in frame_generator: # run inference on the frame result = model.infer(frame)[0] # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) In the above code we\u0026rsquo;re pulling in the object detection yolov8x-640 model from Roboflow Inference, but we could have easily swapped it out for any of our fine tuned models from Roboflow or Roboflow Universe üòÆ.\n# rock, paper, scissors model from roboflow universe model = get_roboflow_model(\u0026#39;rock-paper-scissors-sxsw/11\u0026#39;, api_key=\u0026#39;roboflow_private_api_key\u0026#39;) # custom trained model on tricks my golden retriever Ollie can do model = get_roboflow_model(\u0026#39;goldeneye/8\u0026#39;, api_key=\u0026#39;roboflow_private_api_key\u0026#39;) # Yolov8 Segmentation model from Roboflow model = get_roboflow_model(\u0026#39;yolov8x-seg-640\u0026#39;) We\u0026rsquo;re also using a fancy utility from the Supervision package called get_video_frame_generator which returns a Python generator that yields frames from our video. Lastly, we\u0026rsquo;re running Inference on each frame with our model and converting the results to a Supervision Detections object.\nAnnotations At the time of writing this document Supervision supports 15 different annotation types. These include BoundingBox, BoxCorner, Color, Circle, Dot, Triangle, Ellipse, Halo, Mask, Polygon, Label, Blur, Pixelate, Trace, and HeatMap.\nStacking Annotations Each annoator has the ability to be stacked; however one interesting finding is that the order may influence the quality of the output. For example, if you were to annotate a bounding box, then stack a pixelated annoation, the bounding box may be pixelated as seen in the image below.\nYou can solve this problem by instead adding the pixelated annotation first, then the bounding box annotation.\nLet\u0026rsquo;s dive into the code for creating these annotations.\nimport supervision as sv from inference.models.utils import get_roboflow_model if __name__ == \u0026#39;__main__\u0026#39;: # load the yolov8X model from roboflow inference model = get_roboflow_model(\u0026#39;yolov8x-640\u0026#39;) # get video info from the video path and dynamically generate line thickness video_info = sv.VideoInfo.from_video_path(\u0026#39;vehicle.mp4\u0026#39;) thickness = sv.calculate_dynamic_line_thickness(video_info.resolution_wh) # create a bounding box annotator with dynamic thickness and a pixelate annotator bounding_box = sv.BoundingBoxAnnotator(thickness=thickness) pixalate = sv.PixelateAnnotator() # get frames iterable from video and loop over them frame_generator = sv.get_video_frames_generator(\u0026#39;vehicle.mp4\u0026#39;) # create a video sink context manager to write the annotated frames to with sv.VideoSink(target_path=\u0026#34;output.mp4\u0026#34;, video_info=video_info) as sink: for frame in frame_generator: # run inference on the frame result = model.infer(frame)[0] # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) # apply pixalate on frame copy, then add bounding box annotated_frame = pixalate.annotate(scene=frame.copy(), detections=detections) annotated_frame = bounding_box.annotate(scene=annotated_frame, detections=detections) # save the annotated frame to the video sink sink.write_frame(frame=annotated_frame) We\u0026rsquo;ve introduced a couple of helpful utilities. The VideoInfo object helps us by providing information including frames per second, height, width, etc. We can use this information to dynamically generate annotator line thickness with the sv.calculate_dynamic_line_thickness() method. Next, we create a couple of annotators, a BoundingBoxAnnotator and a PixelateAnnotator. We\u0026rsquo;re also introducing a new concept called a VideoSink which is just a fancy context manager that allows us to write frames to a video output. Prior to this we apply the annotations to the frame with the annotate methods on the annotators. Pretty cool right?\nSegmentation vs Detection Annotations Some annotations require specific model output that depend on the data returned at inference time. More specifically, the Halo, Mask, and Polygon all use sv.Detections.mask under the hood to generate the annotations. This means that we need to use a segmentation model, instead of our object detection model to use these annoations. Let\u0026rsquo;s swap out our model yolov8x-640 for yolov8x-seg-640 to ensure we recieve a mask property in our detection ojects. Take a look at an example of the Polygon Annotator.\nimport supervision as sv from inference.models.utils import get_roboflow_model if __name__ == \u0026#39;__main__\u0026#39;: # load a segmentation model from roboflow inference instead model = get_roboflow_model(\u0026#39;yolov8x-seg-640\u0026#39;) ... Tracking and Annotations The Trace annotator requires the sv.Detections.tracker_id be present to generate annotations. This means that we\u0026rsquo;ll have to use a tracker. Trackers are a piece of code that identifies objects across frames and assigns them a unique id. For example, we could use a tracker to learn what direction a vehicle is moving. There are a few popular trackers at the time of writing this including ByteTrack and Bot-SORT. Supervision makes using trackers a breeze and comes with ByteTrack built-in.\nLet\u0026rsquo;s dive into some tracking code!\nimport supervision as sv from inference.models.utils import get_roboflow_model if __name__ == \u0026#39;__main__\u0026#39;: # load the yolov8X model from roboflow inference model = get_roboflow_model(\u0026#39;yolov8x-seg-640\u0026#39;) # get video info from the video path and dynamically generate line thickness and text_scale video_info = sv.VideoInfo.from_video_path(\u0026#39;vehicle.mp4\u0026#39;) thickness = sv.calculate_dynamic_line_thickness(video_info.resolution_wh) text_scale = sv.calculate_dynamic_text_scale(video_info.resolution_wh) # create a trace and label annotator, with dynamic video info trace = sv.TraceAnnotator(thickness=thickness) label = sv.LabelAnnotator(text_thickness=thickness, text_scale=text_scale) # create a ByteTrack object to track detections byte_tracker = sv.ByteTrack(frame_rate=video_info.fps) # get frames iterable from video and loop over them frame_generator = sv.get_video_frames_generator(\u0026#39;vehicle.mp4\u0026#39;) # create a video sink context manager to write the annotated frames to with sv.VideoSink(target_path=\u0026#34;output.mp4\u0026#34;, video_info=video_info) as sink: for frame in frame_generator: # run inference on the frame result = model.infer(frame)[0] # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) # update detections with tracker ids tracked_detections = byte_tracker.update_with_detections(detections) # apply trace annotator to frame annotated_frame = trace.annotate(scene=frame.copy(), detections=tracked_detections) # create label text for annotator labels = [ f\u0026#34;{tracker_id}\u0026#34; for tracker_id in tracked_detections.tracker_id ] # apply label annotator to frame annotated_frame = label.annotate(scene=annotated_frame, detections=tracked_detections, labels=labels) # save the annotated frame to the video sink sink.write_frame(frame=annotated_frame) In this code, we add text_scale as a new dynamic value for labeling the tracker_id. Next, we add a Trace and Label Annotator with these dynamic values. We also create a ByteTrack object to track our detections passing in the video\u0026rsquo;s frame rate. From there, all we need to do is update the detections with our byte_tracker results using the method update_with_detections. Lastly, we create a list of labels from the tracker_ids and pass them to the label annotator while also utilzing the TraceAnnotator to trace the path of each detection.\nGOTCHA - Stacking Trace with Segmentation Annotations When trying to stack a Trace annotation with a Polygon annotation, I ran into a hiccup. When using a detections from ByteTrack.update_with_detections(detections) the resulting sv.Detections does not include segmentation masks üò≠. This isn\u0026rsquo;t a show stopper, but there is a little bit of nuance if we\u0026rsquo;d like to stack a Segmentation Annotator with a Trace Annotator.\nWe need to apply the polygon annotation with the sv.Detections.from_inference() instead of the detections from ByteTrack.update_with_detections(detections). The former contains the mask and the latter doesn\u0026rsquo;t include the mask. This looks to be a bug, as the latter does include a mask property, it just returns None. More to come on this.\n... # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) # first apply annotation that requires sv.Detections.mask annotated_frame = polygon.annotate(scene=frame.copy(), detections=detections) # then update detections with tracker ids tracked_detections = byte_tracker.update_with_detections(detections) # apply trace annotator to frame annotated_frame = trace.annotate(scene=annotated_frame, detections=tracked_detections) ... To wrap up Annotators, let\u0026rsquo;s get a little crazy üôà. Let\u0026rsquo;s cycle through a few more of the available annotations in a single video to check them all out. Below is the resulting video.\nAnnotations are a powerful tool for visualizing detections. Now let\u0026rsquo;s cover some powerful tools that Supervision provides.\nTools Supervision provides a few additional tools that can be used to filter detections, count detections, create zones, and slice frames. Let\u0026rsquo;s dive into a few of them!\nCounting with Line Zones A Line Zone can be utilized for counting a number of objects that cross a predefined line. One cool feature of the line is that it keeps track of two attributes: in_count and out_count. These attributes can be used to count the number of objects that cross the line in either direction. Note that the LineZone utilizes the sv.Detections.tracker_id to keep track of objects so make sure you\u0026rsquo;re using a tracker as discussed above. Remember earlier when I said that supervision has 15 annotators? Well, I lied. There are a few more that are only documented in in the best kind of documenation, the code üòú. Let\u0026rsquo;s create a LineZone and show it in action with the LineZoneAnnotator.\nAnd here\u0026rsquo;s the code.\nimport supervision as sv from inference.models.utils import get_roboflow_model if __name__ == \u0026#39;__main__\u0026#39;: # load the yolov8X model from roboflow inference model = get_roboflow_model(\u0026#39;yolov8x-640\u0026#39;) # get video info from the video path and dynamically generate line thickness and text_scale video_info = sv.VideoInfo.from_video_path(\u0026#39;vehicle.mp4\u0026#39;) text_scale = sv.calculate_dynamic_text_scale(video_info.resolution_wh) # create a ByteTrack object to track detections byte_tracker = sv.ByteTrack(frame_rate=video_info.fps) # get frames iterable from video and loop over them frame_generator = sv.get_video_frames_generator(\u0026#39;vehicle.mp4\u0026#39;) # create two points of a line for the LineZone start_point = sv.Point(0, video_info.height*(3/4)) end_point = sv.Point(video_info.width, video_info.height*(3/4)) # create the LineZone object line_zone = sv.LineZone(start_point, end_point) # create the LineZoneAnnotator line_annotator = sv.LineZoneAnnotator( color=sv.Color.green(), text_scale=text_scale, custom_in_text=\u0026#34;OUT\u0026#34;, custom_out_text=\u0026#34;IN\u0026#34;, ) # create a video sink context manager to write the annotated frames to with sv.VideoSink(target_path=\u0026#34;output.mp4\u0026#34;, video_info=video_info) as sink: for frame in frame_generator: # run inference on the frame result = model.infer(frame)[0] # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) # update detections with tracker ids tracked_detections = byte_tracker.update_with_detections(detections) # update the linezone object with detections line_zone.trigger(detections=detections) # apply the line zone annotator to the frame annotated_frame = line_annotator.annotate(frame=frame.copy(), line_counter=line_zone) # save the annotated frame to the video sink sink.write_frame(frame=annotated_frame) When first running this code, I ran into the following issue:\nTraceback (most recent call last): File \u0026#34;/Users/nick/git/computer-vision/line_zone.py\u0026#34;, line 41, in \u0026lt;module\u0026gt; line_zone.trigger(detections=detections) File \u0026#34;/Users/nick/git/computer-vision/venv/lib/python3.11/site-packages/supervision/detection/line_counter.py\u0026#34;, line 57, in trigger for i, (xyxy, _, confidence, class_id, tracker_id) in enumerate(detections): ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: too many values to unpack (expected 5) To fix this issue, I cracked open the supervision package and change the following code in line_counter.py to ignore the data field in the detections object.\nfor i, (xyxy, _, confidence, class_id, tracker_id, _) in enumerate(detections): We\u0026rsquo;re using the early release candidate so expect a couple of bugs. This should be fixed in the next release. Let\u0026rsquo;s now move on to another tool, the Polygon Zone!\nFiltering with Polygon Zones The Polygon Zone is an object we can utilize for a variety of tasks. We can filter detections, count, and provide logic for a variety of other tasks. Let\u0026rsquo;s use it to filter out detections that are only on the right side of the highway. We\u0026rsquo;ll also utilize the PolygonZoneAnnotator to visualize the polygon zone and keep a count of detections inside of it.\nLet\u0026rsquo;s take a peek at the code.\nimport supervision as sv from inference.models.utils import get_roboflow_model import numpy as np if __name__ == \u0026#39;__main__\u0026#39;: # load the yolov8X model from roboflow inference model = get_roboflow_model(\u0026#39;yolov8n-640\u0026#39;) # get video info from the video path and dynamically generate line thickness and text_scale video_info = sv.VideoInfo.from_video_path(\u0026#39;vehicle.mp4\u0026#39;) text_scale = sv.calculate_dynamic_text_scale(video_info.resolution_wh) # create a ByteTrack object to track detections byte_tracker = sv.ByteTrack(frame_rate=video_info.fps) # get frames iterable from video and loop over them frame_generator = sv.get_video_frames_generator(\u0026#39;vehicle.mp4\u0026#39;) # create a polygon for use in the PolygonZone using https://roboflow.github.io/polygonzone/ polygon = np.array([[9, 1758],[1125, 846],[1697, 850],[1885, 2146],[17, 2146],[17, 1754]]) # create the PolygonZone object and PolygonZoneAnnotator polygon_zone = sv.PolygonZone(polygon, frame_resolution_wh=video_info.resolution_wh) polygon_annotator = sv.PolygonZoneAnnotator(color=sv.Color.green(), zone=polygon_zone, text_scale=text_scale) # create a box annotator to visualize detections inside the polygon zone box_annotator = sv.BoxAnnotator(text_scale=text_scale) # create a video sink context manager to write the annotated frames to with sv.VideoSink(target_path=\u0026#34;polygonzone.mp4\u0026#34;, video_info=video_info) as sink: for frame in frame_generator: # run inference on the frame result = model.infer(frame)[0] # convert the detections to a supervision detections object detections = sv.Detections.from_inference(result) # filter based on the polygon zone detections = detections[polygon_zone.trigger(detections)] # update detections with tracker ids tracked_detections = byte_tracker.update_with_detections(detections) # update the polygon zone with detections for count polygon_zone.trigger(tracked_detections) # show the polygon zone and box annotator annotated_frame = polygon_annotator.annotate(scene=frame.copy()) annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=tracked_detections) # save the annotated frame to the video sink sink.write_frame(frame=annotated_frame) Holy cow that\u0026rsquo;s pretty sweet! We\u0026rsquo;re using the PolygonZone and PolygonZoneAnnotator to both filter detections and count the vehicles in the zone. You can start to see all the possibilities of using custom models and a variety of polygon zones.\nFirst Impressions It\u0026rsquo;s clear that the package is moving quickly and features are changing rapidly. The documentation is currently lagging behind, so sometimes you need to dive into the code to see what\u0026rsquo;s new. As I started to play with the tool it\u0026rsquo;s easy to see how custom models and this tooling can help developers make the world programmable. It\u0026rsquo;s incredible what you can do in a few lines of code with the Supervision package. I\u0026rsquo;m excited to try out some of the other improvements to Supervision as they are released. The future of computervision is bright and it\u0026rsquo;s only the beginning. Cheers to the future üçª.\n","permalink":"http://localhost:1313/posts/supervision/","summary":"This year I\u0026rsquo;m starting a new role with Roboflow. I\u0026rsquo;ll be helping companies build and deploy computer vision applications. The team, the product, and the mission are all incredible and I\u0026rsquo;m excited to be a part of building the future of computer vision. With this in mind, I thought it\u0026rsquo;d be fun to document my journey as I learn some of the tools and techniques that we use to build computer vision applications \u0026ndash; starting with the Supervision python package.","title":"Supervision - First Impressions"},{"content":"I\u0026rsquo;ve been thinking about writing more. If I\u0026rsquo;m being honest with myself, I\u0026rsquo;ve been considering writing more for a while. However, I haven\u0026rsquo;t been writing, but this year I\u0026rsquo;m changing that. To make this change, I need to confront why I haven\u0026rsquo;t written more in the past and what I hope to gain from writing in the future.\nWhat\u0026rsquo;s been holding my writing back? Impostor syndrome is something I\u0026rsquo;ve struggled with since writing my first Python script 6 years ago. This topic could, and probably should, be a full post in the future. Without a formal computer science education, sometimes my mind tricks me into thinking, \u0026ldquo;Am I really the best person to speak on technology?\u0026rdquo; This thought, combined with a hint of \u0026ldquo;I\u0026rsquo;m not a good writer,\u0026rdquo; has held me back from writing more. So, what\u0026rsquo;s different this time around?\nWhy Write? This time, I\u0026rsquo;m writing for myself. I\u0026rsquo;m writing to improve my writing, solidify my learning, and to share my knowledge with others.\nImprove my writing Writing is important. It\u0026rsquo;s a medium through which we share ideas, learn, and build. I want to share my ideas. I want to help others learn. I want to build important things. Improving my writing will help me achieve these goals.\nSolidify my learning Writing helps me learn. I learn best by doing. Writing about technology, fitness, nutrition, golfing, fishing, and cooking will help me become better at the things I love in life.\nShare my knowledge I\u0026rsquo;ve learned a lot from others sharing their experiences. Now, it\u0026rsquo;s my turn to help others by sharing my knowledge.\nThe 2024 Writing Challenge This afternoon, I texted my good friend Andrew with an idea: a blog challenge.\nThe rules are simple:\nOne post a week, published on Sunday by midnight. So here it goes, my first post of the 2024 writing challenge. Why write? Well, that\u0026rsquo;s really up to you.\n","permalink":"http://localhost:1313/posts/why-write/","summary":"I\u0026rsquo;ve been thinking about writing more. If I\u0026rsquo;m being honest with myself, I\u0026rsquo;ve been considering writing more for a while. However, I haven\u0026rsquo;t been writing, but this year I\u0026rsquo;m changing that. To make this change, I need to confront why I haven\u0026rsquo;t written more in the past and what I hope to gain from writing in the future.\nWhat\u0026rsquo;s been holding my writing back? Impostor syndrome is something I\u0026rsquo;ve struggled with since writing my first Python script 6 years ago.","title":"Why Write?"},{"content":"Welcome Back Folks Welcome back to The Fundamentals blog series. Where we have conversations about fundamental technology concepts in a laid back and non-intimidating fashion. The ultimate goal you might ask? To refresh, learn, grow, and be better engineers. If you haven\u0026rsquo;t read the the fundamentals introduction post, feel free to give it a skim. Let\u0026rsquo;s get started!\nBefore Reading Today we will be using Python3 as our programming language of choice to work through examples and help us learn. If you do not have Python3 installed, follow the official Python docs to install it on your particular computer. To check if you have Python3 run the command from the shell\u0026hellip;\npython3 --version Today\u0026rsquo;s Topic! It\u0026rsquo;s all Ones and Zeroes\nHave you ever heard the phrase, \u0026ldquo;it\u0026rsquo;s all ones and zeroes\u0026rdquo;? When I first heard this phrase I was struggling with writing a program to message me about haircuts. No seriously, our barber in Des Moines has a 2 month wait, so I wanted to get a text when someone cancelled their appointment last minute. At that time I was fairly early in learning about how computers worked under the hood. The friend joking said \u0026ldquo;it can\u0026rsquo;t be that hard, it\u0026rsquo;s all ones and zeroes\u0026rdquo;. At the time, I didn\u0026rsquo;t pay much attention to the comment. But as I continued my career I couldn\u0026rsquo;t shake the comment. Could this really be the case? Could all programs, Instagram feeds, TikTok videos, and cat memes boil down to ones and zeroes? And if so, how could all these complex systems we rely on every day boil down to just two digits?\nLet\u0026rsquo;s count to Ten If I asked you to count to ten, you\u0026rsquo;d probably rattle off.\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\nBut what if I instaed started counting like this\u0026hellip;\n0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, 1010\nOr even more strange\u0026hellip;\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a\nWait a minute, did you just use the letter \u0026lsquo;a\u0026rsquo; instead of \u0026lsquo;10\u0026rsquo;?\nYOUBETCHA!\nAnd the best part, all three of these are correct depending on the context. The context we are missing when I asked you to \u0026ldquo;count to ten\u0026rdquo; is the numeral system.\nNumeral Systems Numeral systems are used as a way to express numbers utilizing digits, letters, and other symbols. When learning to count to 10 in school, the numeral system we were using was the decimal numerial system, also known as base-10 positional numeral system. Woah, when I first learned this was the case, I felt as though I had been living a lie! There are a ton of other extremely important and useful numeral systems being used today.\nTo list a few\u0026hellip;\nUnary (Base-1) Octal (Base-8) Hindu-Arabic (Most Common in the world!) Hexadecimal (Base-16) Decimal (Base-10) Binary (Base-2) Due to a number of factors, the binary or base-2 numerical system is used by almost all computers.\nA Python Example Python, being a higher level language, has a ton of slick built in functions that help increase the productivity of developers. One of which we will utilize to learn about numberal systems is the built in called bin(). This function takes an integer number and converts it into a binary string prefixed with 0b. Let\u0026rsquo;s try it out!\nnumber_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] for number in number_list: print(bin(number)) We first create a list of numbers from 1 to 10. We then loop through that list, printing the binary conversion of the number. And as we expect! The output matches our counting to 10 in binary from above!\n0b1 0b10 0b11 0b100 0b101 0b110 0b111 0b1000 0b1001 0b1010 Test your understanding! Count to 100 in Octal! Count to 100 in Hexadecimal! So how exactly is it that grandma\u0026rsquo;s accidental Facebook statuses condense down to 1s and 0s?\nCharacter Encodings Character encoding are used to take our number systems and provide additional context! The context is in the form of symbols and characters us humans utilize to communicate. There is a lot of interesting history around character encodings, but for now a few common ones out in the wild are\nASCII UTF-8 ISO 8859-1 So if this truly is the case, let\u0026rsquo;s see an recent example that I came across.\nA Python Example Let\u0026rsquo;s revisit the list of built in functions, this time to utilize ord(). This function takes an string representing one Unicode character and returns an integer representing the unicode code point of that character. UTF-8 is defined by the Unicode standard. Let\u0026rsquo;s give it a go.\nsentance = \u0026#39;h√©llo w√µrld\u0026#39; for character in sentance: print(ord(character)) Our output should look something like this, an output of base-10 integers.\n104 233 108 108 111 32 119 245 114 108 100 We can double check our output against the UTF-8 character table.\nTest your understanding! Print the alphabet in unicode Talk nerdy to me Last Valentine\u0026rsquo;s Day, my lovely fiance Megan wrote me an AWESOME card. The front had a digital heart on it and displayed Megan\u0026rsquo;s art skillz.\nI was impressed when I saw it, but soon realized that there was a hidden message lurking on the inside.\nIt appeared to be all ones and zeroes! I assumed that this letter was written in the binary number system! Let\u0026rsquo;s take a look at the message, and attempt to decode it using python!\nA Binary Love Letter The first step, and most tedious was copying the message from written text to text on my computer. I used my favorite text editor VIM to create the file \u0026ldquo;love-note.txt\u0026rdquo;.\n01001101 01111001 00100000 01000100 01100101 01100001 01110010 01100101 01110011 01110100 00100000 01010110 01100001 01101100 01100101 01101110 01110100 01101001 01101110 0110101 00100000 01001001 00100000 01001100 01101111 01110110 01100101 00100000 01011001 01101111 01110101 Now that we have our base-2 numbers in a file, we can read them into our python program.\nwith open(\u0026#34;love-note.txt\u0026#34;) as f: byte_list = f.read().splitlines() We read our file line by line into a list of strings. Next we can use the int() built in function to construct a new list of base-10 numbers.\nnum_list = [int(byte, 2) for byte in byte_list] This format is called a list comprehension. List comprehensions provide a concise way of creating a list. You can read more about this syntax here Anyway, now that we have a list of base-10 integers, let\u0026rsquo;s use the built in chr() function to build a list of unicode characters. Lastly, let\u0026rsquo;s build a sentance from the list with the join method.\nchar_list = [chr(num) for num in num_list] sentance = \u0026#34;\u0026#34;.join(char_list) Our output should now read My Dearest Valentin5 I Love You. Ohhh no\u0026hellip; Megan had a typo\u0026hellip; I guess it can be forgiven due to the fact that she wrote 247 1s and 0s!\nTest your understanding The missing character is an \u0026ldquo;e\u0026rdquo;. What is its base 10 number representation? What about binary? ","permalink":"http://localhost:1313/posts/fundamentals-blog-series/fundamentals-binary/","summary":"Welcome Back Folks Welcome back to The Fundamentals blog series. Where we have conversations about fundamental technology concepts in a laid back and non-intimidating fashion. The ultimate goal you might ask? To refresh, learn, grow, and be better engineers. If you haven\u0026rsquo;t read the the fundamentals introduction post, feel free to give it a skim. Let\u0026rsquo;s get started!\nBefore Reading Today we will be using Python3 as our programming language of choice to work through examples and help us learn.","title":"The Fundamentals - It's all Ones and Zeroes"},{"content":"A few weeks ago a friends partner reached out wanting some advice on learning to program and getting a job in the technology industry.\n\u0026ldquo;I\u0026rsquo;m currently a high school teacher (band director) and am looking to make the change to web development in the next year.\u0026rdquo;\nHe was really excited about starting his career shift from a band director at the local high school to a hacker and software engineer. The problem though, was that he didn\u0026rsquo;t really know where to start.\nHe wanted to pick my brain about two things I\u0026rsquo;ve recently gone though.\nHow does one make a shift from a non technology job to software engineering? Where should I start? When I first saw this email, I honestly had no clue how to respond. Sure, I recently transitioned from a career in manufacturing engineering to software engineering and developer. I was now automating distributed systems with ansible , using time series data to monitor systems with prometheus, and writing self service tools in python with the flask web framework. I didn\u0026rsquo;t really have a game plan at the time though. My game plan looked like giving it a try, and seeing how it worked out. Luckily for myself, I had a lot of time to learn, and some great mentors to help me. But that didn\u0026rsquo;t help the high school band teacher\u0026hellip; I thought about his situation for the next few weeks and tried to answer the questions:\nWhat would I do differently if I was making a career shift to the technology industry? Where would I start? The Fundamentals Blog Series The fundamentals blog series is my answer to the above two questions. It\u0026rsquo;s my attempt to answer the high school band teachers questions about making the shift from non technical work to technical work. The fundamentals is a blog series about computer and technology concepts explained in a digestible and non convoluted format. The blog series is meant to be approachable, humble, and non assuming. We will cover a variety of software engineering and computer science concepts, have a chat about them, and test our understanding together with examples.\nAudience I\u0026rsquo;m writing this blog series for a few different people I\u0026rsquo;ve met over the years.\nThe high school band teacher looking to make a career shift. The 4 year college graduate, wanting a refresher on the fundamentals. The person who thinks computers are too complicated to learn about. The blog series is also for folks just looking to better understand the world around us. It is my hope that more and more people realize that technology is just another language like English or Spanish. Technology and software engineering should not be seen as intimidating. They should be viewed as an exciting tool set in which you can build a new future! you betcha, it takes practice, but think of all the cool stuff we get to build together to make the world a better place!\nThe Goals of Series I have two major goals for the fundamentals blog series.\nMotivate non-traditional developers to break into the technology industry You may have heard via social media, in the news, or from your grandma that many companies including Google and Apple have ditched the college degree requirements. This is pretty freaking exciting if you ask me\u0026hellip; This means you don\u0026rsquo;t have to get into crazy student loan debt to have a really awesome career in tech. Demand is high, and it\u0026rsquo;s your brain they are demanding! Read a few programming books, this blog series(shameless plug), and take a coding boot camp. You\u0026rsquo;ll be well on your way to working in the industry! My plea to you is that you are passionate and devote yourself to learning and the process of learning. Use this blog as one of your many resources!\nHave a conversation about technology concepts in a non-intimidating fashion The blog series was designed to be a conversation between two people about bite sized computer science topics. Imagine that we are grabbing coffee, and while we are slamming that iced americano, or cappuccino we discussed bit shifting and why it\u0026rsquo;s important. I also want to have conversations about why things turned out the way they are Today, commonly known as history. While looking at history i\u0026rsquo;d like you to question the way the world works Today, and think how it could be better Tomorrow. The goal is for the series to be a learning path and to break down the barriers for other folks to join the party. Any yes, it is a freaking party. Let\u0026rsquo;s Party!\nWhat to Expect Before you join me on this journey, lets chat expectations.\nEach blog will be no more than 30 minutes. We are all busy, I get it\u0026hellip; so i\u0026rsquo;ll keep the concepts contained. The content will be short, sweet, and concise. Expect to need around 30 minutes to read the blog post and test your understanding.\nTesting your Understanding Each blog will have an exercise to test your understanding of the main topic. Solutions will be on a separate, slightly hidden page, so try your best to answer the questions before peaking. Testing your understanding is pretty important for these posts, so give it a try!\nNo coding experience needed, but\u0026hellip;. Although coding isn\u0026rsquo;t a prerequisite, we will be reading some code. The plan is to use a easy to read/use programming language to test understanding. I\u0026rsquo;m taking Paul Graham\u0026rsquo;s advice here:\nHow can I learn to program? Find a friend who knows how to program. Get them to set you up with a system where you can edit and run programs. Use whatever language they suggest for a beginner (probably it will be Python or Ruby).\nWe will be using mosty python3 on this journey, but I\u0026rsquo;m also interested in bringing go and ruby into the mix. Are you ready for this?\nLet\u0026rsquo;s do this! ","permalink":"http://localhost:1313/posts/fundamentals-blog-series/fundamentals-intro/","summary":"A few weeks ago a friends partner reached out wanting some advice on learning to program and getting a job in the technology industry.\n\u0026ldquo;I\u0026rsquo;m currently a high school teacher (band director) and am looking to make the change to web development in the next year.\u0026rdquo;\nHe was really excited about starting his career shift from a band director at the local high school to a hacker and software engineer. The problem though, was that he didn\u0026rsquo;t really know where to start.","title":"The Fundamentals - Introduction"}]